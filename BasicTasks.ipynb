{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to demonstrate some basic tasks in graph learning. In general, many of the graph learning problems can fall into the following categories:\n",
    "\n",
    "* node classification: assign a label to a node.\n",
    "* link prediction: predict the existence of an edge between two nodes.\n",
    "* graph classification: assign a label to a graph.\n",
    "\n",
    "There are many real-world applications that can be formulated as one of the graph problems. In fraud detection, we want to predict if a user is a malicious user; we basically assign a binary label to a user. In community detection, we want to predict which community a node belongs to. In recommendation, we want to predict if a user will purchase an item. If he/she does, there is an edge between the user and the item. Thus, recommendation is a link prediction task. In drug discovery, we want to predict the property of a molecule, which can be considered as a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL works with different deep learning frameworks. In this tutorial, we show how DGL works with Pytorch. Thus, let's first load Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load DGL. When we load DGL, we need to set the DGL backend. The DGL backend is one of the deep learning frameworks. Currently, DGL has Pytorch and MXNet backends. Because this tutorial develops models in Pytorch, we have to set the DGL backend to Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import citegrh\n",
    "from dgl.nn.pytorch import conv\n",
    "\n",
    "# Load Pytorch as backend\n",
    "dgl.load_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply GNN to a graph problem, we typically use GNN to compute meaningful node embeddings and then use the node embeddings to perform the downstream task. Thus, the first step is to develop a GNN model to compute node embeddings.\n",
    "\n",
    "DGL provides an nn module that contains many common GNN layers so that we can develop a GNN model with the layers easily. Later, we'll show how to develop a customized GNN model with DGL's message passing interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use [GraphSage](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf), one of the first inductive GNN model. Each GraphSage layer performs the following computation on every node $v$ in the graph:\n",
    "\n",
    "$$h_{N(v)}^k \\gets AGGREGATE_k({h_u^{k-1}, \\forall u \\in N(v)})$$\n",
    "$$h_v^k \\gets \\sigma(W^k \\cdot CONCAT(h_v^{k-1}, h_{N(v)}^k))$$\n",
    "\n",
    ", where $N(v)$ is the neighborhood of node $v$ and $k$ is the layer Id.\n",
    "\n",
    "The GraphSage model stacks multiple layers so that a node $v$ can access neighbors within $k$ hops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(conv.SAGEConv(in_feats, n_hidden, aggregator_type,\n",
    "                                         feat_drop=dropout, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(conv.SAGEConv(n_hidden, n_hidden, aggregator_type,\n",
    "                                             feat_drop=dropout, activation=activation))\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "In this tutorial, we use a citation network called pubmed for demonstration. DGL has a large collection of built-in datasets. Please see [this doc]() for more information.\n",
    "\n",
    "A node in the citation network is a paper and an edge represents the citation between two papers. There are 19,717 papers in the dataset and the total number of citations between the papers is 88,651.\n",
    "\n",
    "When the dataset is loaded into DGL, the network structure is in a [NetworkX]() object. NetworkX is a very popular Python graph library. It provides comprehensive API for graph manipulation and is very useful for preprocessing small graphs.\n",
    "\n",
    "All other graph data, such as node features and labels, are stored in NumPy tensors. When we load the tensors, we convert them to Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess dataset\n",
    "data = citegrh.load_pubmed()\n",
    "features = torch.FloatTensor(data.features)\n",
    "labels = torch.LongTensor(data.labels)\n",
    "train_mask = torch.BoolTensor(data.train_mask)\n",
    "val_mask = torch.BoolTensor(data.val_mask)\n",
    "test_mask = torch.BoolTensor(data.test_mask)\n",
    "in_feats = features.shape[1]\n",
    "n_classes = data.num_labels\n",
    "n_edges = data.graph.number_of_edges()\n",
    "print(\"\"\"----Data statistics------'\n",
    "      #Edges %d\n",
    "      #Classes %d\n",
    "      #Train samples %d\n",
    "      #Val samples %d\n",
    "      #Test samples %d\"\"\" %\n",
    "          (n_edges, n_classes,\n",
    "           train_mask.sum().item(),\n",
    "           val_mask.sum().item(),\n",
    "           test_mask.sum().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the graph structure is stored in a NetworkX object, we can use NetworkX to preprocess the graph. In this example, we remove all self-loops in the graph.\n",
    "\n",
    "Then we create a DGLGraph from the grpah dataset and convert it to a read-only DGLGraph, which supports more efficient computation. Currently, any sampling API in DGL only works on read-only DGLGraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = data.graph\n",
    "g.remove_edges_from(g.selfloop_edges())\n",
    "g = DGLGraph(g)\n",
    "g.readonly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node classification in the semi-supervised setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is how to perform node classification in a semi-supervised setting. That is, we have the entire graph structure and all node features. We only have labels on some of the nodes. We want to predict the labels on other nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a 2-layer GraphSage model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_hidden = 64\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "aggregator_type = 'gcn'\n",
    "\n",
    "gconv_model = GraphSAGE(in_feats,\n",
    "                        n_hidden,\n",
    "                        n_layers,\n",
    "                        F.relu,\n",
    "                        dropout,\n",
    "                        aggregator_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the node classification model based on the GraphSage model. The GraphSage model takes a DGLGraph object and node features as input and computes node embeddings as output. With node embeddings, we use a cross entry loss to train the node classification model.\n",
    "\n",
    "An interested user can try passing any GNN model to the node classification model to compute node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassification(nn.Module):\n",
    "    def __init__(self, gconv_model, n_hidden, n_classes):\n",
    "        super(NodeClassification, self).__init__()\n",
    "        self.gconv_model = gconv_model\n",
    "        self.fc = nn.Linear(n_hidden, n_classes, bias=True)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        embs = self.gconv_model(g, features)\n",
    "        logits = self.fc(embs)\n",
    "        return logits\n",
    "\n",
    "# Node classification task\n",
    "model = NodeClassification(gconv_model, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining a model for node classification, we also need to define an evaluation function to evaluate the performance of a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, test_mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[test_mask]\n",
    "        test_labels = labels[test_mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == test_labels)\n",
    "        return correct.item() * 1.0 / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model and evaluation function, we can put everything to the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    # forward\n",
    "    logits = model(g, features)\n",
    "    loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(model, g, features, labels, val_mask)\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | \"\n",
    "            \"ETputs(KTEPS) {:.2f}\".format(epoch, np.mean(dur), loss.item(),\n",
    "                                            acc, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "print()\n",
    "acc = evaluate(model, g, features, labels, test_mask)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link prediction is to predict the existence of an edge between two nodes. In a general case, we consider that an edge connects two similar nodes. Thus, link prediction is basically to find pairs of similar nodes.\n",
    "\n",
    "Traditionally, one of the most commonly methods for link prediction is SVD. We first use SVD to compute node embeddings and use the embeddings to predict the connection between nodes. The problem with SVD is that it only takes the graph structure into account for link prediction. In many cases, node features provide a lot of information.\n",
    "\n",
    "When using GNN for link prediction, we can take both the graph structure and node features into consideration when evaluating node similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the node classification task, we first use GraphSage as the base model to compute node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model hyperparameters\n",
    "n_hidden = 16\n",
    "n_layers = 1\n",
    "\n",
    "# create GraphSAGE model\n",
    "gconv_model = GraphSAGE(in_feats,\n",
    "                        n_hidden,\n",
    "                        n_layers,\n",
    "                        F.relu,\n",
    "                        dropout,\n",
    "                        aggregator_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike node classification, which uses node labels as training signal, link prediction simply uses the graph structure as the training signal. We consider nodes connected by edges are similar, while nodes not connected by edges are dissimilar.\n",
    "\n",
    "Thus, the first thing we need to do is to generate positive samples (i.e., pairs of nodes that are connected by edges) and negative edges (i.e., pairs of nodes that are not connected by edges). We should train on each positive sample with multiple negative samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL provides an edge sampler `EdgeSampler`, which generates positive edge samples and negative edge samples efficiently. We can use this edge sampler to easily generate a batch of positive edges and negative edges. `neg_sample` samples one tenth of the edges in the graph as positive edges and generate `neg_sample_size` negative edges for each positive edge. It generates two subgraphs. The positive subgraph contains all positive edges sampled from the graph `g`, while the negative subgraph contains all negative edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample(g, neg_sample_size, edges=None, return_false_neg=True):\n",
    "    sampler = dgl.contrib.sampling.EdgeSampler(g, batch_size=int(g.number_of_edges()/10),\n",
    "                                               seed_edges=edges,\n",
    "                                               neg_sample_size=neg_sample_size,\n",
    "                                               negative_mode='tail',\n",
    "                                               shuffle=True,\n",
    "                                               return_false_neg=return_false_neg)\n",
    "    sampler = iter(sampler)\n",
    "    return next(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having positive edge subgraphs and negative edge subgraphs, we can now compute the similarity on the positive edge samples and negative edge samples.\n",
    "\n",
    "In this tutorial, we use cosine similarity to measure the similarity between two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_score_func(g, emb):\n",
    "    pos_src, pos_dst = g.all_edges(order='eid')\n",
    "    # Get the node Ids in the parent graph.\n",
    "    pos_src = g.parent_nid[pos_src]\n",
    "    pos_dst = g.parent_nid[pos_dst]\n",
    "    # Read the node embeddings of the source nodes and destination nodes.\n",
    "    pos_heads = emb[pos_src]\n",
    "    pos_tails = emb[pos_dst]\n",
    "    # cosine similarity\n",
    "    return torch.sum(pos_heads * pos_tails, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the similarity of source nodes and destination nodes on negative edge samples. Because each positive edge is paired with `neg_sample_size`, we reshape the the node embeddings into a 3D tensor of shape (batch_size, neg_sample_size, n_hidden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_score_func(g, emb, neg_sample_size):\n",
    "    neg_src, neg_dst = g.all_edges(order='eid')\n",
    "    neg_src = g.parent_nid[neg_src]\n",
    "    neg_dst = g.parent_nid[neg_dst]\n",
    "    neg_heads = emb[neg_src].reshape(-1, neg_sample_size, emb.shape[1])\n",
    "    neg_tails = emb[neg_dst].reshape(-1, neg_sample_size, emb.shape[1])\n",
    "    assert neg_heads.shape[0] == neg_tails.shape[0]\n",
    "    return torch.sum(neg_heads * neg_tails, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put everything together. We first use GraphSage to compute node embeddings. After having the node embeddings, we compute the similarity scores on positive edge samples and negative edge samples. Then we use the following loss function on a positive edge and the corresponding negative edges:\n",
    "\n",
    "$$L = -log(\\sigma(z_u^T z_v)) - Q \\cdot E_{v_n\\~P_n(v)}(log(\\sigma(-z_u^T z_{v_n})))$$\n",
    "\n",
    "With this loss, training should increase the similarity scores on the positive edges and decrease the similarity scores on negative edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCE loss\n",
    "def NCE_loss(pos_score, neg_score):\n",
    "    pos_score = F.logsigmoid(pos_score)\n",
    "    neg_score = F.logsigmoid(-neg_score)\n",
    "    return -pos_score - torch.sum(neg_score, dim=1)\n",
    "\n",
    "class LinkPrediction(nn.Module):\n",
    "    def __init__(self, gconv_model):\n",
    "        super(LinkPrediction, self).__init__()\n",
    "        self.gconv_model = gconv_model\n",
    "\n",
    "    def forward(self, g, features, neg_sample_size):\n",
    "        emb = self.gconv_model(g, features)\n",
    "        pos_g, neg_g = neg_sample(g, neg_sample_size, return_false_neg=False)\n",
    "        pos_score = pos_score_func(pos_g, emb)\n",
    "        neg_score = neg_score_func(neg_g, emb, neg_sample_size)\n",
    "        return torch.mean(NCE_loss(pos_score, neg_score))\n",
    "    \n",
    "model = LinkPrediction(gconv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like node classification, we define an evaluation function to evaluate the training result. Ideally, the similarity score of a positive edge should be higher than all negative edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(gconv_model, g, features, neg_sample_size):\n",
    "    gconv_model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb = gconv_model(g, features)\n",
    "        \n",
    "        pos_g, neg_g = neg_sample(g, neg_sample_size, test_eids, return_false_neg=True)\n",
    "        pos_score = pos_score_func(pos_g, emb)\n",
    "        neg_score = neg_score_func(neg_g, emb, neg_sample_size)\n",
    "        filter_bias = neg_g.edata['false_neg'].reshape(-1, neg_sample_size)\n",
    "\n",
    "        pos_score = F.logsigmoid(pos_score)\n",
    "        neg_score = F.logsigmoid(neg_score)\n",
    "        neg_score -= filter_bias.float()\n",
    "        pos_score = pos_score.unsqueeze(1)\n",
    "        rankings = torch.sum(neg_score >= pos_score, dim=1) + 1\n",
    "        return emb, np.mean(1.0/rankings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to the actual training.\n",
    "\n",
    "We first split the graph into the training set and the testing set. We random sample 80\\% edges as the training data and 20\\% edges as the testing data. There is no overlap between the training set and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eids = np.random.permutation(g.number_of_edges())\n",
    "train_eids = eids[:int(len(eids) * 0.8)]\n",
    "test_eids = eids[int(len(eids) * 0.8):]\n",
    "train_g = g.edge_subgraph(train_eids, preserve_nodes=True)\n",
    "test_g = g.edge_subgraph(test_eids, preserve_nodes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "weight_decay = 5e-4\n",
    "n_epochs = 50\n",
    "lr = 1e-3\n",
    "neg_sample_size = 100\n",
    "\n",
    "# use optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# initialize graph\n",
    "dur = []\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "    # forward\n",
    "    loss = model(g, features, neg_sample_size)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        _, acc = evaluate(gconv_model, g, features, neg_sample_size)\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f}\"\n",
    "              .format(epoch, np.mean(dur), loss.item(), acc))\n",
    "    else:\n",
    "        print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f}\"\n",
    "              .format(epoch, np.mean(dur), loss.item()))\n",
    "\n",
    "print()\n",
    "emb, acc = evaluate(gconv_model, g, features, neg_sample_size)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
