{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Graph Classification with DGL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate how to use DGL to finish graph classification tasks. The dataset we use here is Tox21, a public database measuring toxicity of compounds.  The dataset contains qualitative toxicity measurements for 8014 compounds on 12 different targets, including nuclear receptors and stress response pathways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import TUDataset\n",
    "from dgl.data.utils import split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from dgl.nn.pytorch import conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./asset/enzymes.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = TUDataset(\"ENZYMES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DGLGraph(num_nodes=37, num_edges=168,\n",
       "         ndata_schemes={'node_labels': Scheme(shape=(1,), dtype=torch.int64), 'node_attr': Scheme(shape=(18,), dtype=torch.int64)}\n",
       "         edata_schemes={})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.graph_labels=torch.tensor(dataset.graph_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i][0].ndata['node_attr']= (dataset[i][0].ndata['node_attr']).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset = split_dataset(dataset, [0.8, 0.2], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGLGraph(num_nodes=37, num_edges=168,\n",
      "         ndata_schemes={'node_labels': Scheme(shape=(1,), dtype=torch.int64), 'node_attr': Scheme(shape=(18,), dtype=torch.float32)}\n",
      "         edata_schemes={})\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "graph, label= dataset[0]\n",
    "print(graph)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL could batch multiple small graphs together to accelerate the computation. Detail of batching can be found [here](https://docs.dgl.ai/tutorials/basics/4_batch.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/batch.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_molgraphs_for_classification(data):\n",
    "    \"\"\"Batching a list of datapoints for dataloader in classification tasks.\"\"\"\n",
    "    graphs, labels = map(list, zip(*data))\n",
    "    bg = dgl.batch(graphs)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    return bg, labels\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=512,\n",
    "                          collate_fn=collate_molgraphs_for_classification)\n",
    "val_loader = DataLoader(valset, batch_size=512,\n",
    "                        collate_fn=collate_molgraphs_for_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a two layer Graph Convolutional Network to classify the graphs. Detailed source code can be found [here](https://github.com/dmlc/dgl/blob/master/python/dgl/model_zoo/chem/classifiers.py#L111)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 out_feats):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            conv.GraphConv(in_feats, n_hidden, activation=F.relu),\n",
    "            conv.GraphConv(n_hidden, n_hidden, activation=F.relu),\n",
    "            conv.GraphConv(n_hidden, n_hidden, activation=F.relu)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Linear(n_hidden, out_feats)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        with g.local_scope():\n",
    "            g.ndata['feat'] = h\n",
    "            h_g = dgl.sum_nodes(g, 'feat')\n",
    "        return self.classifier(h_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNModel(\n",
      "  (layers): ModuleList(\n",
      "    (0): GraphConv(in=18, out=64, normalization=True, activation=<function relu at 0x7f59a57ac0d0>)\n",
      "    (1): GraphConv(in=64, out=64, normalization=True, activation=<function relu at 0x7f59a57ac0d0>)\n",
      "    (2): GraphConv(in=64, out=64, normalization=True, activation=<function relu at 0x7f59a57ac0d0>)\n",
      "  )\n",
      "  (classifier): Linear(in_features=64, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GCNModel(in_feats=18, n_hidden=64, out_feats=6).to(device)\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 6])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss: 128.8334 | Accuracy: 0.1729\n",
      "Epoch 00001 | Loss: 97.0797 | Accuracy: 0.1729\n",
      "Epoch 00002 | Loss: 67.1622 | Accuracy: 0.1729\n",
      "Epoch 00003 | Loss: 53.3588 | Accuracy: 0.1750\n",
      "Epoch 00004 | Loss: 43.3624 | Accuracy: 0.1750\n",
      "Epoch 00005 | Loss: 29.1658 | Accuracy: 0.2062\n",
      "Epoch 00006 | Loss: 14.4205 | Accuracy: 0.1562\n",
      "Epoch 00007 | Loss: 13.5954 | Accuracy: 0.2104\n",
      "Epoch 00008 | Loss: 14.7965 | Accuracy: 0.1958\n",
      "Epoch 00009 | Loss: 18.5140 | Accuracy: 0.2250\n",
      "Epoch 00010 | Loss: 20.7691 | Accuracy: 0.1688\n",
      "Epoch 00011 | Loss: 21.9511 | Accuracy: 0.1625\n",
      "Epoch 00012 | Loss: 21.5492 | Accuracy: 0.1771\n",
      "Epoch 00013 | Loss: 19.2594 | Accuracy: 0.1708\n",
      "Epoch 00014 | Loss: 17.2495 | Accuracy: 0.2167\n",
      "Epoch 00015 | Loss: 14.5592 | Accuracy: 0.2125\n",
      "Epoch 00016 | Loss: 14.0101 | Accuracy: 0.1938\n",
      "Epoch 00017 | Loss: 14.0666 | Accuracy: 0.1917\n",
      "Epoch 00018 | Loss: 12.1471 | Accuracy: 0.1875\n",
      "Epoch 00019 | Loss: 13.1340 | Accuracy: 0.1979\n",
      "Epoch 00020 | Loss: 13.1660 | Accuracy: 0.1792\n",
      "Epoch 00021 | Loss: 11.8353 | Accuracy: 0.1625\n",
      "Epoch 00022 | Loss: 9.5357 | Accuracy: 0.1542\n",
      "Epoch 00023 | Loss: 6.4208 | Accuracy: 0.2375\n",
      "Epoch 00024 | Loss: 5.8358 | Accuracy: 0.2250\n",
      "Epoch 00025 | Loss: 5.4997 | Accuracy: 0.2188\n",
      "Epoch 00026 | Loss: 6.8472 | Accuracy: 0.2417\n",
      "Epoch 00027 | Loss: 7.6919 | Accuracy: 0.2604\n",
      "Epoch 00028 | Loss: 8.2227 | Accuracy: 0.2313\n",
      "Epoch 00029 | Loss: 7.7949 | Accuracy: 0.2396\n",
      "Epoch 00030 | Loss: 7.1335 | Accuracy: 0.2125\n",
      "Epoch 00031 | Loss: 5.7977 | Accuracy: 0.2042\n",
      "Epoch 00032 | Loss: 4.8836 | Accuracy: 0.2542\n",
      "Epoch 00033 | Loss: 5.9723 | Accuracy: 0.2167\n",
      "Epoch 00034 | Loss: 5.8177 | Accuracy: 0.2146\n",
      "Epoch 00035 | Loss: 5.1575 | Accuracy: 0.1812\n",
      "Epoch 00036 | Loss: 4.1910 | Accuracy: 0.2000\n",
      "Epoch 00037 | Loss: 3.5664 | Accuracy: 0.1896\n",
      "Epoch 00038 | Loss: 4.5859 | Accuracy: 0.1979\n",
      "Epoch 00039 | Loss: 4.6811 | Accuracy: 0.2167\n",
      "Epoch 00040 | Loss: 3.8535 | Accuracy: 0.2313\n",
      "Epoch 00041 | Loss: 3.7061 | Accuracy: 0.1896\n",
      "Epoch 00042 | Loss: 3.2758 | Accuracy: 0.2229\n",
      "Epoch 00043 | Loss: 3.7432 | Accuracy: 0.1917\n",
      "Epoch 00044 | Loss: 3.4492 | Accuracy: 0.2208\n",
      "Epoch 00045 | Loss: 3.2442 | Accuracy: 0.1979\n",
      "Epoch 00046 | Loss: 3.1960 | Accuracy: 0.2000\n",
      "Epoch 00047 | Loss: 2.9605 | Accuracy: 0.2021\n",
      "Epoch 00048 | Loss: 2.9444 | Accuracy: 0.2062\n",
      "Epoch 00049 | Loss: 2.9589 | Accuracy: 0.2458\n",
      "Epoch 00050 | Loss: 2.9320 | Accuracy: 0.2458\n",
      "Epoch 00051 | Loss: 2.7533 | Accuracy: 0.2542\n",
      "Epoch 00052 | Loss: 2.8249 | Accuracy: 0.2458\n",
      "Epoch 00053 | Loss: 2.6599 | Accuracy: 0.2250\n",
      "Epoch 00054 | Loss: 2.5008 | Accuracy: 0.2271\n",
      "Epoch 00055 | Loss: 2.6025 | Accuracy: 0.2333\n",
      "Epoch 00056 | Loss: 2.6130 | Accuracy: 0.2521\n",
      "Epoch 00057 | Loss: 2.3749 | Accuracy: 0.2479\n",
      "Epoch 00058 | Loss: 2.3425 | Accuracy: 0.2521\n",
      "Epoch 00059 | Loss: 2.3795 | Accuracy: 0.2500\n",
      "Epoch 00060 | Loss: 2.2265 | Accuracy: 0.2417\n",
      "Epoch 00061 | Loss: 2.2654 | Accuracy: 0.2313\n",
      "Epoch 00062 | Loss: 2.2527 | Accuracy: 0.2354\n",
      "Epoch 00063 | Loss: 2.1208 | Accuracy: 0.2708\n",
      "Epoch 00064 | Loss: 2.0670 | Accuracy: 0.2958\n",
      "Epoch 00065 | Loss: 2.1611 | Accuracy: 0.2667\n",
      "Epoch 00066 | Loss: 2.0197 | Accuracy: 0.2875\n",
      "Epoch 00067 | Loss: 2.0880 | Accuracy: 0.2604\n",
      "Epoch 00068 | Loss: 2.0253 | Accuracy: 0.2750\n",
      "Epoch 00069 | Loss: 2.0272 | Accuracy: 0.2854\n",
      "Epoch 00070 | Loss: 2.0329 | Accuracy: 0.2875\n",
      "Epoch 00071 | Loss: 2.0109 | Accuracy: 0.2917\n",
      "Epoch 00072 | Loss: 1.9498 | Accuracy: 0.2958\n",
      "Epoch 00073 | Loss: 2.0097 | Accuracy: 0.2583\n",
      "Epoch 00074 | Loss: 1.9275 | Accuracy: 0.2812\n",
      "Epoch 00075 | Loss: 1.9574 | Accuracy: 0.2979\n",
      "Epoch 00076 | Loss: 1.9232 | Accuracy: 0.2958\n",
      "Epoch 00077 | Loss: 1.9094 | Accuracy: 0.2958\n",
      "Epoch 00078 | Loss: 1.9144 | Accuracy: 0.2792\n",
      "Epoch 00079 | Loss: 1.8992 | Accuracy: 0.2917\n",
      "Epoch 00080 | Loss: 1.8799 | Accuracy: 0.2979\n",
      "Epoch 00081 | Loss: 1.8989 | Accuracy: 0.2896\n",
      "Epoch 00082 | Loss: 1.8690 | Accuracy: 0.2896\n",
      "Epoch 00083 | Loss: 1.8799 | Accuracy: 0.2812\n",
      "Epoch 00084 | Loss: 1.8656 | Accuracy: 0.2729\n",
      "Epoch 00085 | Loss: 1.8647 | Accuracy: 0.2875\n",
      "Epoch 00086 | Loss: 1.8594 | Accuracy: 0.2917\n",
      "Epoch 00087 | Loss: 1.8500 | Accuracy: 0.2833\n",
      "Epoch 00088 | Loss: 1.8443 | Accuracy: 0.2812\n",
      "Epoch 00089 | Loss: 1.8403 | Accuracy: 0.2771\n",
      "Epoch 00090 | Loss: 1.8315 | Accuracy: 0.2812\n",
      "Epoch 00091 | Loss: 1.8282 | Accuracy: 0.2792\n",
      "Epoch 00092 | Loss: 1.8177 | Accuracy: 0.2792\n",
      "Epoch 00093 | Loss: 1.8186 | Accuracy: 0.2958\n",
      "Epoch 00094 | Loss: 1.8082 | Accuracy: 0.2750\n",
      "Epoch 00095 | Loss: 1.8055 | Accuracy: 0.2792\n",
      "Epoch 00096 | Loss: 1.8010 | Accuracy: 0.2812\n",
      "Epoch 00097 | Loss: 1.7955 | Accuracy: 0.2938\n",
      "Epoch 00098 | Loss: 1.7917 | Accuracy: 0.2938\n",
      "Epoch 00099 | Loss: 1.7865 | Accuracy: 0.2917\n",
      "Epoch 00100 | Loss: 1.7816 | Accuracy: 0.2812\n",
      "Epoch 00101 | Loss: 1.7786 | Accuracy: 0.2896\n",
      "Epoch 00102 | Loss: 1.7717 | Accuracy: 0.2938\n",
      "Epoch 00103 | Loss: 1.7701 | Accuracy: 0.2917\n",
      "Epoch 00104 | Loss: 1.7637 | Accuracy: 0.2854\n",
      "Epoch 00105 | Loss: 1.7616 | Accuracy: 0.2833\n",
      "Epoch 00106 | Loss: 1.7562 | Accuracy: 0.2875\n",
      "Epoch 00107 | Loss: 1.7533 | Accuracy: 0.2896\n",
      "Epoch 00108 | Loss: 1.7496 | Accuracy: 0.2958\n",
      "Epoch 00109 | Loss: 1.7452 | Accuracy: 0.2833\n",
      "Epoch 00110 | Loss: 1.7423 | Accuracy: 0.2812\n",
      "Epoch 00111 | Loss: 1.7378 | Accuracy: 0.2917\n",
      "Epoch 00112 | Loss: 1.7352 | Accuracy: 0.2938\n",
      "Epoch 00113 | Loss: 1.7305 | Accuracy: 0.2917\n",
      "Epoch 00114 | Loss: 1.7279 | Accuracy: 0.2875\n",
      "Epoch 00115 | Loss: 1.7237 | Accuracy: 0.2875\n",
      "Epoch 00116 | Loss: 1.7205 | Accuracy: 0.2938\n",
      "Epoch 00117 | Loss: 1.7166 | Accuracy: 0.3000\n",
      "Epoch 00118 | Loss: 1.7131 | Accuracy: 0.2938\n",
      "Epoch 00119 | Loss: 1.7098 | Accuracy: 0.2896\n",
      "Epoch 00120 | Loss: 1.7059 | Accuracy: 0.2979\n",
      "Epoch 00121 | Loss: 1.7027 | Accuracy: 0.2958\n",
      "Epoch 00122 | Loss: 1.6991 | Accuracy: 0.2979\n",
      "Epoch 00123 | Loss: 1.6958 | Accuracy: 0.3000\n",
      "Epoch 00124 | Loss: 1.6923 | Accuracy: 0.2958\n",
      "Epoch 00125 | Loss: 1.6891 | Accuracy: 0.2979\n",
      "Epoch 00126 | Loss: 1.6858 | Accuracy: 0.2979\n",
      "Epoch 00127 | Loss: 1.6828 | Accuracy: 0.3000\n",
      "Epoch 00128 | Loss: 1.6796 | Accuracy: 0.3042\n",
      "Epoch 00129 | Loss: 1.6766 | Accuracy: 0.3021\n",
      "Epoch 00130 | Loss: 1.6731 | Accuracy: 0.3000\n",
      "Epoch 00131 | Loss: 1.6700 | Accuracy: 0.3000\n",
      "Epoch 00132 | Loss: 1.6667 | Accuracy: 0.3021\n",
      "Epoch 00133 | Loss: 1.6634 | Accuracy: 0.3042\n",
      "Epoch 00134 | Loss: 1.6601 | Accuracy: 0.3000\n",
      "Epoch 00135 | Loss: 1.6567 | Accuracy: 0.2979\n",
      "Epoch 00136 | Loss: 1.6531 | Accuracy: 0.2979\n",
      "Epoch 00137 | Loss: 1.6495 | Accuracy: 0.3021\n",
      "Epoch 00138 | Loss: 1.6456 | Accuracy: 0.3042\n",
      "Epoch 00139 | Loss: 1.6415 | Accuracy: 0.3104\n",
      "Epoch 00140 | Loss: 1.6376 | Accuracy: 0.3125\n",
      "Epoch 00141 | Loss: 1.6338 | Accuracy: 0.3146\n",
      "Epoch 00142 | Loss: 1.6302 | Accuracy: 0.3146\n",
      "Epoch 00143 | Loss: 1.6264 | Accuracy: 0.3167\n",
      "Epoch 00144 | Loss: 1.6227 | Accuracy: 0.3208\n",
      "Epoch 00145 | Loss: 1.6191 | Accuracy: 0.3312\n",
      "Epoch 00146 | Loss: 1.6156 | Accuracy: 0.3333\n",
      "Epoch 00147 | Loss: 1.6122 | Accuracy: 0.3354\n",
      "Epoch 00148 | Loss: 1.6088 | Accuracy: 0.3354\n",
      "Epoch 00149 | Loss: 1.6053 | Accuracy: 0.3354\n",
      "Epoch 00150 | Loss: 1.6016 | Accuracy: 0.3333\n",
      "Epoch 00151 | Loss: 1.5979 | Accuracy: 0.3354\n",
      "Epoch 00152 | Loss: 1.5945 | Accuracy: 0.3312\n",
      "Epoch 00153 | Loss: 1.5914 | Accuracy: 0.3312\n",
      "Epoch 00154 | Loss: 1.5883 | Accuracy: 0.3417\n",
      "Epoch 00155 | Loss: 1.5854 | Accuracy: 0.3458\n",
      "Epoch 00156 | Loss: 1.5828 | Accuracy: 0.3458\n",
      "Epoch 00157 | Loss: 1.5801 | Accuracy: 0.3438\n",
      "Epoch 00158 | Loss: 1.5770 | Accuracy: 0.3479\n",
      "Epoch 00159 | Loss: 1.5734 | Accuracy: 0.3438\n",
      "Epoch 00160 | Loss: 1.5708 | Accuracy: 0.3354\n",
      "Epoch 00161 | Loss: 1.5684 | Accuracy: 0.3458\n",
      "Epoch 00162 | Loss: 1.5663 | Accuracy: 0.3479\n",
      "Epoch 00163 | Loss: 1.5633 | Accuracy: 0.3417\n",
      "Epoch 00164 | Loss: 1.5600 | Accuracy: 0.3479\n",
      "Epoch 00165 | Loss: 1.5572 | Accuracy: 0.3500\n",
      "Epoch 00166 | Loss: 1.5549 | Accuracy: 0.3479\n",
      "Epoch 00167 | Loss: 1.5525 | Accuracy: 0.3542\n",
      "Epoch 00168 | Loss: 1.5497 | Accuracy: 0.3479\n",
      "Epoch 00169 | Loss: 1.5469 | Accuracy: 0.3521\n",
      "Epoch 00170 | Loss: 1.5433 | Accuracy: 0.3479\n",
      "Epoch 00171 | Loss: 1.5392 | Accuracy: 0.3542\n",
      "Epoch 00172 | Loss: 1.5367 | Accuracy: 0.3542\n",
      "Epoch 00173 | Loss: 1.5346 | Accuracy: 0.3625\n",
      "Epoch 00174 | Loss: 1.5337 | Accuracy: 0.3604\n",
      "Epoch 00175 | Loss: 1.5334 | Accuracy: 0.3604\n",
      "Epoch 00176 | Loss: 1.5431 | Accuracy: 0.3625\n",
      "Epoch 00177 | Loss: 1.5296 | Accuracy: 0.3688\n",
      "Epoch 00178 | Loss: 1.5268 | Accuracy: 0.3667\n",
      "Epoch 00179 | Loss: 1.5340 | Accuracy: 0.3812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00180 | Loss: 1.5261 | Accuracy: 0.3688\n",
      "Epoch 00181 | Loss: 1.5203 | Accuracy: 0.3812\n",
      "Epoch 00182 | Loss: 1.5212 | Accuracy: 0.4000\n",
      "Epoch 00183 | Loss: 1.5202 | Accuracy: 0.3729\n",
      "Epoch 00184 | Loss: 1.5155 | Accuracy: 0.3604\n",
      "Epoch 00185 | Loss: 1.5127 | Accuracy: 0.3896\n",
      "Epoch 00186 | Loss: 1.5100 | Accuracy: 0.3688\n",
      "Epoch 00187 | Loss: 1.5126 | Accuracy: 0.3771\n",
      "Epoch 00188 | Loss: 1.5066 | Accuracy: 0.3896\n",
      "Epoch 00189 | Loss: 1.5051 | Accuracy: 0.3812\n",
      "Epoch 00190 | Loss: 1.5020 | Accuracy: 0.4000\n",
      "Epoch 00191 | Loss: 1.4996 | Accuracy: 0.3896\n",
      "Epoch 00192 | Loss: 1.5005 | Accuracy: 0.3812\n",
      "Epoch 00193 | Loss: 1.4912 | Accuracy: 0.4062\n",
      "Epoch 00194 | Loss: 1.4918 | Accuracy: 0.4042\n",
      "Epoch 00195 | Loss: 1.4924 | Accuracy: 0.3979\n",
      "Epoch 00196 | Loss: 1.4865 | Accuracy: 0.4167\n",
      "Epoch 00197 | Loss: 1.4818 | Accuracy: 0.4229\n",
      "Epoch 00198 | Loss: 1.4849 | Accuracy: 0.4125\n",
      "Epoch 00199 | Loss: 1.4817 | Accuracy: 0.4188\n",
      "Epoch 00200 | Loss: 1.4752 | Accuracy: 0.4146\n",
      "Epoch 00201 | Loss: 1.4743 | Accuracy: 0.4250\n",
      "Epoch 00202 | Loss: 1.4714 | Accuracy: 0.4229\n",
      "Epoch 00203 | Loss: 1.4696 | Accuracy: 0.4167\n",
      "Epoch 00204 | Loss: 1.4775 | Accuracy: 0.4021\n",
      "Epoch 00205 | Loss: 1.5374 | Accuracy: 0.4021\n",
      "Epoch 00206 | Loss: 1.4761 | Accuracy: 0.4146\n",
      "Epoch 00207 | Loss: 1.5782 | Accuracy: 0.3417\n",
      "Epoch 00208 | Loss: 1.7172 | Accuracy: 0.3896\n",
      "Epoch 00209 | Loss: 1.6828 | Accuracy: 0.3583\n",
      "Epoch 00210 | Loss: 1.5929 | Accuracy: 0.3292\n",
      "Epoch 00211 | Loss: 1.7669 | Accuracy: 0.3146\n",
      "Epoch 00212 | Loss: 2.0838 | Accuracy: 0.3729\n",
      "Epoch 00213 | Loss: 1.9330 | Accuracy: 0.3438\n",
      "Epoch 00214 | Loss: 2.1210 | Accuracy: 0.2542\n",
      "Epoch 00215 | Loss: 1.9096 | Accuracy: 0.2417\n",
      "Epoch 00216 | Loss: 2.0483 | Accuracy: 0.3667\n",
      "Epoch 00217 | Loss: 2.5436 | Accuracy: 0.3250\n",
      "Epoch 00218 | Loss: 1.7878 | Accuracy: 0.3458\n",
      "Epoch 00219 | Loss: 2.2234 | Accuracy: 0.2875\n",
      "Epoch 00220 | Loss: 2.4375 | Accuracy: 0.2896\n",
      "Epoch 00221 | Loss: 2.1404 | Accuracy: 0.3854\n",
      "Epoch 00222 | Loss: 2.0410 | Accuracy: 0.3333\n",
      "Epoch 00223 | Loss: 2.0489 | Accuracy: 0.2938\n",
      "Epoch 00224 | Loss: 1.9556 | Accuracy: 0.3229\n",
      "Epoch 00225 | Loss: 2.1980 | Accuracy: 0.2667\n",
      "Epoch 00226 | Loss: 1.8852 | Accuracy: 0.3083\n",
      "Epoch 00227 | Loss: 2.4767 | Accuracy: 0.2375\n",
      "Epoch 00228 | Loss: 2.3763 | Accuracy: 0.3500\n",
      "Epoch 00229 | Loss: 2.5815 | Accuracy: 0.3792\n",
      "Epoch 00230 | Loss: 2.2062 | Accuracy: 0.3625\n",
      "Epoch 00231 | Loss: 2.3201 | Accuracy: 0.2917\n",
      "Epoch 00232 | Loss: 1.6280 | Accuracy: 0.3604\n",
      "Epoch 00233 | Loss: 2.0747 | Accuracy: 0.2583\n",
      "Epoch 00234 | Loss: 2.1819 | Accuracy: 0.2750\n",
      "Epoch 00235 | Loss: 2.1816 | Accuracy: 0.3292\n",
      "Epoch 00236 | Loss: 2.3718 | Accuracy: 0.3312\n",
      "Epoch 00237 | Loss: 2.3543 | Accuracy: 0.2875\n",
      "Epoch 00238 | Loss: 2.1754 | Accuracy: 0.3208\n",
      "Epoch 00239 | Loss: 2.4041 | Accuracy: 0.3250\n",
      "Epoch 00240 | Loss: 2.1478 | Accuracy: 0.2917\n",
      "Epoch 00241 | Loss: 2.1528 | Accuracy: 0.2896\n",
      "Epoch 00242 | Loss: 2.0264 | Accuracy: 0.3292\n",
      "Epoch 00243 | Loss: 1.8329 | Accuracy: 0.3104\n",
      "Epoch 00244 | Loss: 1.9747 | Accuracy: 0.2771\n",
      "Epoch 00245 | Loss: 2.0526 | Accuracy: 0.3250\n",
      "Epoch 00246 | Loss: 1.7378 | Accuracy: 0.3812\n",
      "Epoch 00247 | Loss: 1.8980 | Accuracy: 0.3479\n",
      "Epoch 00248 | Loss: 2.1484 | Accuracy: 0.2854\n",
      "Epoch 00249 | Loss: 1.5340 | Accuracy: 0.3937\n",
      "Epoch 00250 | Loss: 1.8300 | Accuracy: 0.3604\n",
      "Epoch 00251 | Loss: 1.9052 | Accuracy: 0.4000\n",
      "Epoch 00252 | Loss: 1.6896 | Accuracy: 0.4000\n",
      "Epoch 00253 | Loss: 1.6164 | Accuracy: 0.3521\n",
      "Epoch 00254 | Loss: 1.8111 | Accuracy: 0.2875\n",
      "Epoch 00255 | Loss: 1.6288 | Accuracy: 0.3875\n",
      "Epoch 00256 | Loss: 1.6181 | Accuracy: 0.4229\n",
      "Epoch 00257 | Loss: 1.6952 | Accuracy: 0.3375\n",
      "Epoch 00258 | Loss: 1.4932 | Accuracy: 0.4375\n",
      "Epoch 00259 | Loss: 1.6591 | Accuracy: 0.3646\n",
      "Epoch 00260 | Loss: 1.5759 | Accuracy: 0.4396\n",
      "Epoch 00261 | Loss: 1.4653 | Accuracy: 0.4417\n",
      "Epoch 00262 | Loss: 1.6593 | Accuracy: 0.3104\n",
      "Epoch 00263 | Loss: 1.4654 | Accuracy: 0.4396\n",
      "Epoch 00264 | Loss: 1.5596 | Accuracy: 0.4062\n",
      "Epoch 00265 | Loss: 1.5100 | Accuracy: 0.4146\n",
      "Epoch 00266 | Loss: 1.4793 | Accuracy: 0.4229\n",
      "Epoch 00267 | Loss: 1.5225 | Accuracy: 0.3896\n",
      "Epoch 00268 | Loss: 1.4527 | Accuracy: 0.4542\n",
      "Epoch 00269 | Loss: 1.4883 | Accuracy: 0.4188\n",
      "Epoch 00270 | Loss: 1.4737 | Accuracy: 0.3833\n",
      "Epoch 00271 | Loss: 1.4473 | Accuracy: 0.4271\n",
      "Epoch 00272 | Loss: 1.4768 | Accuracy: 0.4667\n",
      "Epoch 00273 | Loss: 1.4459 | Accuracy: 0.4667\n",
      "Epoch 00274 | Loss: 1.4534 | Accuracy: 0.3958\n",
      "Epoch 00275 | Loss: 1.4385 | Accuracy: 0.4125\n",
      "Epoch 00276 | Loss: 1.4359 | Accuracy: 0.4500\n",
      "Epoch 00277 | Loss: 1.4522 | Accuracy: 0.4604\n",
      "Epoch 00278 | Loss: 1.4142 | Accuracy: 0.4688\n",
      "Epoch 00279 | Loss: 1.4466 | Accuracy: 0.4042\n",
      "Epoch 00280 | Loss: 1.4232 | Accuracy: 0.4542\n",
      "Epoch 00281 | Loss: 1.4219 | Accuracy: 0.4833\n",
      "Epoch 00282 | Loss: 1.4268 | Accuracy: 0.4646\n",
      "Epoch 00283 | Loss: 1.4152 | Accuracy: 0.4688\n",
      "Epoch 00284 | Loss: 1.4249 | Accuracy: 0.4604\n",
      "Epoch 00285 | Loss: 1.4062 | Accuracy: 0.4688\n",
      "Epoch 00286 | Loss: 1.4214 | Accuracy: 0.4646\n",
      "Epoch 00287 | Loss: 1.4092 | Accuracy: 0.4646\n",
      "Epoch 00288 | Loss: 1.4088 | Accuracy: 0.4667\n",
      "Epoch 00289 | Loss: 1.4101 | Accuracy: 0.4625\n",
      "Epoch 00290 | Loss: 1.4048 | Accuracy: 0.4813\n",
      "Epoch 00291 | Loss: 1.4062 | Accuracy: 0.4792\n",
      "Epoch 00292 | Loss: 1.4004 | Accuracy: 0.4750\n",
      "Epoch 00293 | Loss: 1.4043 | Accuracy: 0.4667\n",
      "Epoch 00294 | Loss: 1.3992 | Accuracy: 0.4833\n",
      "Epoch 00295 | Loss: 1.3980 | Accuracy: 0.4792\n",
      "Epoch 00296 | Loss: 1.3997 | Accuracy: 0.4833\n",
      "Epoch 00297 | Loss: 1.3947 | Accuracy: 0.4875\n",
      "Epoch 00298 | Loss: 1.3971 | Accuracy: 0.4792\n",
      "Epoch 00299 | Loss: 1.3923 | Accuracy: 0.4833\n",
      "Epoch 00300 | Loss: 1.3954 | Accuracy: 0.4729\n",
      "Epoch 00301 | Loss: 1.3906 | Accuracy: 0.4833\n",
      "Epoch 00302 | Loss: 1.3920 | Accuracy: 0.4813\n",
      "Epoch 00303 | Loss: 1.3902 | Accuracy: 0.4771\n",
      "Epoch 00304 | Loss: 1.3893 | Accuracy: 0.4833\n",
      "Epoch 00305 | Loss: 1.3886 | Accuracy: 0.4813\n",
      "Epoch 00306 | Loss: 1.3871 | Accuracy: 0.4875\n",
      "Epoch 00307 | Loss: 1.3872 | Accuracy: 0.4729\n",
      "Epoch 00308 | Loss: 1.3849 | Accuracy: 0.4917\n",
      "Epoch 00309 | Loss: 1.3853 | Accuracy: 0.4854\n",
      "Epoch 00310 | Loss: 1.3836 | Accuracy: 0.4875\n",
      "Epoch 00311 | Loss: 1.3830 | Accuracy: 0.4792\n",
      "Epoch 00312 | Loss: 1.3820 | Accuracy: 0.4917\n",
      "Epoch 00313 | Loss: 1.3812 | Accuracy: 0.4917\n",
      "Epoch 00314 | Loss: 1.3805 | Accuracy: 0.4792\n",
      "Epoch 00315 | Loss: 1.3793 | Accuracy: 0.4833\n",
      "Epoch 00316 | Loss: 1.3791 | Accuracy: 0.4875\n",
      "Epoch 00317 | Loss: 1.3776 | Accuracy: 0.4875\n",
      "Epoch 00318 | Loss: 1.3773 | Accuracy: 0.4792\n",
      "Epoch 00319 | Loss: 1.3761 | Accuracy: 0.4854\n",
      "Epoch 00320 | Loss: 1.3757 | Accuracy: 0.4875\n",
      "Epoch 00321 | Loss: 1.3745 | Accuracy: 0.4917\n",
      "Epoch 00322 | Loss: 1.3741 | Accuracy: 0.4875\n",
      "Epoch 00323 | Loss: 1.3731 | Accuracy: 0.4917\n",
      "Epoch 00324 | Loss: 1.3723 | Accuracy: 0.4917\n",
      "Epoch 00325 | Loss: 1.3716 | Accuracy: 0.4917\n",
      "Epoch 00326 | Loss: 1.3708 | Accuracy: 0.4896\n",
      "Epoch 00327 | Loss: 1.3701 | Accuracy: 0.4896\n",
      "Epoch 00328 | Loss: 1.3692 | Accuracy: 0.4917\n",
      "Epoch 00329 | Loss: 1.3686 | Accuracy: 0.4875\n",
      "Epoch 00330 | Loss: 1.3677 | Accuracy: 0.4917\n",
      "Epoch 00331 | Loss: 1.3671 | Accuracy: 0.4875\n",
      "Epoch 00332 | Loss: 1.3663 | Accuracy: 0.4917\n",
      "Epoch 00333 | Loss: 1.3655 | Accuracy: 0.4958\n",
      "Epoch 00334 | Loss: 1.3648 | Accuracy: 0.4938\n",
      "Epoch 00335 | Loss: 1.3641 | Accuracy: 0.4917\n",
      "Epoch 00336 | Loss: 1.3633 | Accuracy: 0.4917\n",
      "Epoch 00337 | Loss: 1.3626 | Accuracy: 0.4917\n",
      "Epoch 00338 | Loss: 1.3619 | Accuracy: 0.5000\n",
      "Epoch 00339 | Loss: 1.3611 | Accuracy: 0.4958\n",
      "Epoch 00340 | Loss: 1.3604 | Accuracy: 0.4917\n",
      "Epoch 00341 | Loss: 1.3597 | Accuracy: 0.4958\n",
      "Epoch 00342 | Loss: 1.3590 | Accuracy: 0.4958\n",
      "Epoch 00343 | Loss: 1.3582 | Accuracy: 0.4958\n",
      "Epoch 00344 | Loss: 1.3575 | Accuracy: 0.4979\n",
      "Epoch 00345 | Loss: 1.3567 | Accuracy: 0.4958\n",
      "Epoch 00346 | Loss: 1.3560 | Accuracy: 0.4979\n",
      "Epoch 00347 | Loss: 1.3553 | Accuracy: 0.4979\n",
      "Epoch 00348 | Loss: 1.3546 | Accuracy: 0.4979\n",
      "Epoch 00349 | Loss: 1.3538 | Accuracy: 0.4958\n",
      "Epoch 00350 | Loss: 1.3531 | Accuracy: 0.4979\n",
      "Epoch 00351 | Loss: 1.3523 | Accuracy: 0.5000\n",
      "Epoch 00352 | Loss: 1.3516 | Accuracy: 0.5042\n",
      "Epoch 00353 | Loss: 1.3509 | Accuracy: 0.5042\n",
      "Epoch 00354 | Loss: 1.3502 | Accuracy: 0.5042\n",
      "Epoch 00355 | Loss: 1.3494 | Accuracy: 0.5042\n",
      "Epoch 00356 | Loss: 1.3487 | Accuracy: 0.5083\n",
      "Epoch 00357 | Loss: 1.3480 | Accuracy: 0.5083\n",
      "Epoch 00358 | Loss: 1.3472 | Accuracy: 0.5062\n",
      "Epoch 00359 | Loss: 1.3465 | Accuracy: 0.5062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00360 | Loss: 1.3458 | Accuracy: 0.5042\n",
      "Epoch 00361 | Loss: 1.3451 | Accuracy: 0.5083\n",
      "Epoch 00362 | Loss: 1.3443 | Accuracy: 0.5062\n",
      "Epoch 00363 | Loss: 1.3436 | Accuracy: 0.5083\n",
      "Epoch 00364 | Loss: 1.3429 | Accuracy: 0.5042\n",
      "Epoch 00365 | Loss: 1.3422 | Accuracy: 0.5021\n",
      "Epoch 00366 | Loss: 1.3414 | Accuracy: 0.5021\n",
      "Epoch 00367 | Loss: 1.3407 | Accuracy: 0.5021\n",
      "Epoch 00368 | Loss: 1.3400 | Accuracy: 0.5062\n",
      "Epoch 00369 | Loss: 1.3393 | Accuracy: 0.5042\n",
      "Epoch 00370 | Loss: 1.3385 | Accuracy: 0.5083\n",
      "Epoch 00371 | Loss: 1.3378 | Accuracy: 0.5104\n",
      "Epoch 00372 | Loss: 1.3371 | Accuracy: 0.5104\n",
      "Epoch 00373 | Loss: 1.3363 | Accuracy: 0.5083\n",
      "Epoch 00374 | Loss: 1.3356 | Accuracy: 0.5083\n",
      "Epoch 00375 | Loss: 1.3349 | Accuracy: 0.5083\n",
      "Epoch 00376 | Loss: 1.3341 | Accuracy: 0.5083\n",
      "Epoch 00377 | Loss: 1.3334 | Accuracy: 0.5083\n",
      "Epoch 00378 | Loss: 1.3326 | Accuracy: 0.5104\n",
      "Epoch 00379 | Loss: 1.3319 | Accuracy: 0.5125\n",
      "Epoch 00380 | Loss: 1.3312 | Accuracy: 0.5125\n",
      "Epoch 00381 | Loss: 1.3304 | Accuracy: 0.5146\n",
      "Epoch 00382 | Loss: 1.3297 | Accuracy: 0.5146\n",
      "Epoch 00383 | Loss: 1.3289 | Accuracy: 0.5146\n",
      "Epoch 00384 | Loss: 1.3282 | Accuracy: 0.5146\n",
      "Epoch 00385 | Loss: 1.3275 | Accuracy: 0.5167\n",
      "Epoch 00386 | Loss: 1.3268 | Accuracy: 0.5167\n",
      "Epoch 00387 | Loss: 1.3260 | Accuracy: 0.5167\n",
      "Epoch 00388 | Loss: 1.3253 | Accuracy: 0.5167\n",
      "Epoch 00389 | Loss: 1.3245 | Accuracy: 0.5167\n",
      "Epoch 00390 | Loss: 1.3238 | Accuracy: 0.5167\n",
      "Epoch 00391 | Loss: 1.3231 | Accuracy: 0.5167\n",
      "Epoch 00392 | Loss: 1.3223 | Accuracy: 0.5167\n",
      "Epoch 00393 | Loss: 1.3215 | Accuracy: 0.5167\n",
      "Epoch 00394 | Loss: 1.3207 | Accuracy: 0.5146\n",
      "Epoch 00395 | Loss: 1.3200 | Accuracy: 0.5146\n",
      "Epoch 00396 | Loss: 1.3192 | Accuracy: 0.5146\n",
      "Epoch 00397 | Loss: 1.3185 | Accuracy: 0.5146\n",
      "Epoch 00398 | Loss: 1.3177 | Accuracy: 0.5146\n",
      "Epoch 00399 | Loss: 1.3169 | Accuracy: 0.5146\n",
      "Epoch 00400 | Loss: 1.3161 | Accuracy: 0.5146\n",
      "Epoch 00401 | Loss: 1.3154 | Accuracy: 0.5167\n",
      "Epoch 00402 | Loss: 1.3146 | Accuracy: 0.5188\n",
      "Epoch 00403 | Loss: 1.3137 | Accuracy: 0.5188\n",
      "Epoch 00404 | Loss: 1.3129 | Accuracy: 0.5188\n",
      "Epoch 00405 | Loss: 1.3121 | Accuracy: 0.5188\n",
      "Epoch 00406 | Loss: 1.3113 | Accuracy: 0.5167\n",
      "Epoch 00407 | Loss: 1.3104 | Accuracy: 0.5188\n",
      "Epoch 00408 | Loss: 1.3096 | Accuracy: 0.5188\n",
      "Epoch 00409 | Loss: 1.3088 | Accuracy: 0.5188\n",
      "Epoch 00410 | Loss: 1.3080 | Accuracy: 0.5208\n",
      "Epoch 00411 | Loss: 1.3072 | Accuracy: 0.5229\n",
      "Epoch 00412 | Loss: 1.3064 | Accuracy: 0.5250\n",
      "Epoch 00413 | Loss: 1.3056 | Accuracy: 0.5250\n",
      "Epoch 00414 | Loss: 1.3048 | Accuracy: 0.5292\n",
      "Epoch 00415 | Loss: 1.3040 | Accuracy: 0.5292\n",
      "Epoch 00416 | Loss: 1.3032 | Accuracy: 0.5292\n",
      "Epoch 00417 | Loss: 1.3025 | Accuracy: 0.5292\n",
      "Epoch 00418 | Loss: 1.3017 | Accuracy: 0.5292\n",
      "Epoch 00419 | Loss: 1.3010 | Accuracy: 0.5312\n",
      "Epoch 00420 | Loss: 1.3002 | Accuracy: 0.5292\n",
      "Epoch 00421 | Loss: 1.2995 | Accuracy: 0.5292\n",
      "Epoch 00422 | Loss: 1.2988 | Accuracy: 0.5292\n",
      "Epoch 00423 | Loss: 1.2980 | Accuracy: 0.5292\n",
      "Epoch 00424 | Loss: 1.2972 | Accuracy: 0.5271\n",
      "Epoch 00425 | Loss: 1.2964 | Accuracy: 0.5271\n",
      "Epoch 00426 | Loss: 1.2957 | Accuracy: 0.5271\n",
      "Epoch 00427 | Loss: 1.2949 | Accuracy: 0.5271\n",
      "Epoch 00428 | Loss: 1.2941 | Accuracy: 0.5250\n",
      "Epoch 00429 | Loss: 1.2934 | Accuracy: 0.5250\n",
      "Epoch 00430 | Loss: 1.2926 | Accuracy: 0.5250\n",
      "Epoch 00431 | Loss: 1.2918 | Accuracy: 0.5271\n",
      "Epoch 00432 | Loss: 1.2911 | Accuracy: 0.5271\n",
      "Epoch 00433 | Loss: 1.2903 | Accuracy: 0.5271\n",
      "Epoch 00434 | Loss: 1.2896 | Accuracy: 0.5271\n",
      "Epoch 00435 | Loss: 1.2889 | Accuracy: 0.5271\n",
      "Epoch 00436 | Loss: 1.2881 | Accuracy: 0.5271\n",
      "Epoch 00437 | Loss: 1.2874 | Accuracy: 0.5271\n",
      "Epoch 00438 | Loss: 1.2866 | Accuracy: 0.5271\n",
      "Epoch 00439 | Loss: 1.2859 | Accuracy: 0.5271\n",
      "Epoch 00440 | Loss: 1.2851 | Accuracy: 0.5292\n",
      "Epoch 00441 | Loss: 1.2844 | Accuracy: 0.5292\n",
      "Epoch 00442 | Loss: 1.2836 | Accuracy: 0.5292\n",
      "Epoch 00443 | Loss: 1.2829 | Accuracy: 0.5292\n",
      "Epoch 00444 | Loss: 1.2821 | Accuracy: 0.5292\n",
      "Epoch 00445 | Loss: 1.2814 | Accuracy: 0.5271\n",
      "Epoch 00446 | Loss: 1.2807 | Accuracy: 0.5271\n",
      "Epoch 00447 | Loss: 1.2799 | Accuracy: 0.5271\n",
      "Epoch 00448 | Loss: 1.2792 | Accuracy: 0.5271\n",
      "Epoch 00449 | Loss: 1.2784 | Accuracy: 0.5271\n",
      "Epoch 00450 | Loss: 1.2777 | Accuracy: 0.5271\n",
      "Epoch 00451 | Loss: 1.2769 | Accuracy: 0.5271\n",
      "Epoch 00452 | Loss: 1.2762 | Accuracy: 0.5250\n",
      "Epoch 00453 | Loss: 1.2754 | Accuracy: 0.5250\n",
      "Epoch 00454 | Loss: 1.2747 | Accuracy: 0.5250\n",
      "Epoch 00455 | Loss: 1.2739 | Accuracy: 0.5271\n",
      "Epoch 00456 | Loss: 1.2730 | Accuracy: 0.5271\n",
      "Epoch 00457 | Loss: 1.2719 | Accuracy: 0.5271\n",
      "Epoch 00458 | Loss: 1.2706 | Accuracy: 0.5250\n",
      "Epoch 00459 | Loss: 1.2686 | Accuracy: 0.5271\n",
      "Epoch 00460 | Loss: 1.2669 | Accuracy: 0.5229\n",
      "Epoch 00461 | Loss: 1.2671 | Accuracy: 0.5188\n",
      "Epoch 00462 | Loss: 1.2664 | Accuracy: 0.5292\n",
      "Epoch 00463 | Loss: 1.2647 | Accuracy: 0.5292\n",
      "Epoch 00464 | Loss: 1.2640 | Accuracy: 0.5229\n",
      "Epoch 00465 | Loss: 1.2632 | Accuracy: 0.5312\n",
      "Epoch 00466 | Loss: 1.2623 | Accuracy: 0.5292\n",
      "Epoch 00467 | Loss: 1.2609 | Accuracy: 0.5146\n",
      "Epoch 00468 | Loss: 1.2606 | Accuracy: 0.5208\n",
      "Epoch 00469 | Loss: 1.2595 | Accuracy: 0.5208\n",
      "Epoch 00470 | Loss: 1.2583 | Accuracy: 0.5312\n",
      "Epoch 00471 | Loss: 1.2576 | Accuracy: 0.5229\n",
      "Epoch 00472 | Loss: 1.2568 | Accuracy: 0.5250\n",
      "Epoch 00473 | Loss: 1.2559 | Accuracy: 0.5312\n",
      "Epoch 00474 | Loss: 1.2549 | Accuracy: 0.5250\n",
      "Epoch 00475 | Loss: 1.2544 | Accuracy: 0.5229\n",
      "Epoch 00476 | Loss: 1.2533 | Accuracy: 0.5271\n",
      "Epoch 00477 | Loss: 1.2524 | Accuracy: 0.5312\n",
      "Epoch 00478 | Loss: 1.2517 | Accuracy: 0.5250\n",
      "Epoch 00479 | Loss: 1.2510 | Accuracy: 0.5292\n",
      "Epoch 00480 | Loss: 1.2500 | Accuracy: 0.5271\n",
      "Epoch 00481 | Loss: 1.2492 | Accuracy: 0.5271\n",
      "Epoch 00482 | Loss: 1.2484 | Accuracy: 0.5250\n",
      "Epoch 00483 | Loss: 1.2475 | Accuracy: 0.5292\n",
      "Epoch 00484 | Loss: 1.2470 | Accuracy: 0.5250\n",
      "Epoch 00485 | Loss: 1.2461 | Accuracy: 0.5250\n",
      "Epoch 00486 | Loss: 1.2453 | Accuracy: 0.5312\n",
      "Epoch 00487 | Loss: 1.2445 | Accuracy: 0.5271\n",
      "Epoch 00488 | Loss: 1.2437 | Accuracy: 0.5250\n",
      "Epoch 00489 | Loss: 1.2429 | Accuracy: 0.5271\n",
      "Epoch 00490 | Loss: 1.2420 | Accuracy: 0.5333\n",
      "Epoch 00491 | Loss: 1.2413 | Accuracy: 0.5312\n",
      "Epoch 00492 | Loss: 1.2406 | Accuracy: 0.5333\n",
      "Epoch 00493 | Loss: 1.2401 | Accuracy: 0.5333\n",
      "Epoch 00494 | Loss: 1.2398 | Accuracy: 0.5312\n",
      "Epoch 00495 | Loss: 1.2388 | Accuracy: 0.5333\n",
      "Epoch 00496 | Loss: 1.2379 | Accuracy: 0.5292\n",
      "Epoch 00497 | Loss: 1.2370 | Accuracy: 0.5292\n",
      "Epoch 00498 | Loss: 1.2363 | Accuracy: 0.5333\n",
      "Epoch 00499 | Loss: 1.2354 | Accuracy: 0.5292\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    loss_list = []\n",
    "    true_samples = 0\n",
    "    num_samples = 0\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "        bg, labels = batch_data\n",
    "        atom_feats = bg.ndata.pop('node_attr').float()\n",
    "        atom_feats, labels = atom_feats.to(device), \\\n",
    "                                   labels.to(device).squeeze(-1)\n",
    "        logits = model(bg, atom_feats)\n",
    "        loss = loss_criterion(logits, labels)\n",
    "        true_samples += (logits.argmax(1)==labels.long()).float().sum().item()\n",
    "        num_samples += len(labels)\n",
    "        loss_list.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch {:05d} | Loss: {:.4f} | Accuracy: {:.4f}\".format(i, np.mean(loss_list), true_samples/num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48333333333333334\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "true_samples = 0\n",
    "num_samples = 0\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch_data in enumerate(val_loader):\n",
    "        bg, labels = batch_data\n",
    "        atom_feats = bg.ndata.pop('node_attr')\n",
    "        atom_feats, labels = atom_feats.to(device), \\\n",
    "                                   labels.to(device).squeeze(-1)\n",
    "        logits = model(bg, atom_feats)\n",
    "        logits.argmax()\n",
    "        num_samples += len(labels)\n",
    "        true_samples += (logits.argmax(1)==labels.long()).float().sum().item()\n",
    "print(true_samples/num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGL_devday",
   "language": "python",
   "name": "dgl_devday"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
