{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Graph Classification with DGL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate how to use DGL to finish graph classification tasks. The dataset we use here is Tox21, a public database measuring toxicity of compounds.  The dataset contains qualitative toxicity measurements for 8014 compounds on 12 different targets, including nuclear receptors and stress response pathways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import TUDataset\n",
    "from dgl.data.utils import split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from dgl.nn.pytorch import conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./asset/enzymes.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = TUDataset(\"ENZYMES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.graph_labels=torch.tensor(dataset.graph_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i][0].ndata['feat']= torch.tensor(dataset[i][0].ndata['feat']).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset = split_dataset(dataset, [0.8, 0.2], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGLGraph(num_nodes=37, num_edges=168,\n",
      "         ndata_schemes={'feat': Scheme(shape=(18,), dtype=torch.float32)}\n",
      "         edata_schemes={})\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "graph, label= dataset[0]\n",
    "print(graph)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL could batch multiple small graphs together to accelerate the computation. Detail of batching can be found [here](https://docs.dgl.ai/tutorials/basics/4_batch.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/batch.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_molgraphs_for_classification(data):\n",
    "    \"\"\"Batching a list of datapoints for dataloader in classification tasks.\"\"\"\n",
    "    graphs, labels = map(list, zip(*data))\n",
    "    bg = dgl.batch(graphs)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    return bg, labels\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=512,\n",
    "                          collate_fn=collate_molgraphs_for_classification)\n",
    "val_loader = DataLoader(valset, batch_size=512,\n",
    "                        collate_fn=collate_molgraphs_for_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a two layer Graph Convolutional Network to classify the graphs. Detailed source code can be found [here](https://github.com/dmlc/dgl/blob/master/python/dgl/model_zoo/chem/classifiers.py#L111)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 out_feats):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            conv.GraphConv(in_feats, n_hidden, activation=F.relu),\n",
    "            conv.GraphConv(n_hidden, n_hidden, activation=F.relu),\n",
    "            conv.GraphConv(n_hidden, n_hidden, activation=F.relu)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Linear(n_hidden, out_feats)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        with g.local_scope():\n",
    "            g.ndata['feat'] = h\n",
    "            h_g = dgl.sum_nodes(g, 'feat')\n",
    "        return self.classifier(h_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNModel(\n",
      "  (layers): ModuleList(\n",
      "    (0): GraphConv(in=18, out=128, normalization=True, activation=<function relu at 0x116a00158>)\n",
      "    (1): GraphConv(in=128, out=64, normalization=True, activation=<function relu at 0x116a00158>)\n",
      "    (2): GraphConv(in=64, out=32, normalization=True, activation=<function relu at 0x116a00158>)\n",
      "  )\n",
      "  (classifier): Linear(in_features=32, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GCNModel(in_feats=18, n_hidden=64, out_feats=6).to(device)\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss: 78.3070 | Accuracy: 0.1542\n",
      "Epoch 00001 | Loss: 52.2662 | Accuracy: 0.1583\n",
      "Epoch 00002 | Loss: 35.6786 | Accuracy: 0.1875\n",
      "Epoch 00003 | Loss: 26.5348 | Accuracy: 0.1792\n",
      "Epoch 00004 | Loss: 17.2299 | Accuracy: 0.1938\n",
      "Epoch 00005 | Loss: 9.4017 | Accuracy: 0.2271\n",
      "Epoch 00006 | Loss: 4.8281 | Accuracy: 0.2479\n",
      "Epoch 00007 | Loss: 5.1498 | Accuracy: 0.2188\n",
      "Epoch 00008 | Loss: 4.4797 | Accuracy: 0.2604\n",
      "Epoch 00009 | Loss: 4.5233 | Accuracy: 0.2250\n",
      "Epoch 00010 | Loss: 4.2406 | Accuracy: 0.1938\n",
      "Epoch 00011 | Loss: 3.9234 | Accuracy: 0.2271\n",
      "Epoch 00012 | Loss: 3.5610 | Accuracy: 0.2604\n",
      "Epoch 00013 | Loss: 3.4678 | Accuracy: 0.2292\n",
      "Epoch 00014 | Loss: 3.2297 | Accuracy: 0.2229\n",
      "Epoch 00015 | Loss: 2.9896 | Accuracy: 0.2417\n",
      "Epoch 00016 | Loss: 2.8344 | Accuracy: 0.2479\n",
      "Epoch 00017 | Loss: 2.6362 | Accuracy: 0.2021\n",
      "Epoch 00018 | Loss: 2.6142 | Accuracy: 0.1875\n",
      "Epoch 00019 | Loss: 2.5773 | Accuracy: 0.1854\n",
      "Epoch 00020 | Loss: 2.5366 | Accuracy: 0.1917\n",
      "Epoch 00021 | Loss: 2.3635 | Accuracy: 0.2521\n",
      "Epoch 00022 | Loss: 2.3977 | Accuracy: 0.2271\n",
      "Epoch 00023 | Loss: 2.3704 | Accuracy: 0.2167\n",
      "Epoch 00024 | Loss: 2.2661 | Accuracy: 0.2396\n",
      "Epoch 00025 | Loss: 2.1510 | Accuracy: 0.2271\n",
      "Epoch 00026 | Loss: 2.1459 | Accuracy: 0.2062\n",
      "Epoch 00027 | Loss: 2.1566 | Accuracy: 0.2562\n",
      "Epoch 00028 | Loss: 2.0856 | Accuracy: 0.2583\n",
      "Epoch 00029 | Loss: 2.0301 | Accuracy: 0.1917\n",
      "Epoch 00030 | Loss: 2.0279 | Accuracy: 0.2229\n",
      "Epoch 00031 | Loss: 2.0406 | Accuracy: 0.2250\n",
      "Epoch 00032 | Loss: 1.9976 | Accuracy: 0.2208\n",
      "Epoch 00033 | Loss: 1.9706 | Accuracy: 0.1896\n",
      "Epoch 00034 | Loss: 1.9655 | Accuracy: 0.2208\n",
      "Epoch 00035 | Loss: 1.9496 | Accuracy: 0.2271\n",
      "Epoch 00036 | Loss: 1.9355 | Accuracy: 0.2375\n",
      "Epoch 00037 | Loss: 1.8861 | Accuracy: 0.2396\n",
      "Epoch 00038 | Loss: 1.9012 | Accuracy: 0.2188\n",
      "Epoch 00039 | Loss: 1.8801 | Accuracy: 0.2229\n",
      "Epoch 00040 | Loss: 1.8615 | Accuracy: 0.2479\n",
      "Epoch 00041 | Loss: 1.8536 | Accuracy: 0.2521\n",
      "Epoch 00042 | Loss: 1.8411 | Accuracy: 0.2313\n",
      "Epoch 00043 | Loss: 1.8430 | Accuracy: 0.2208\n",
      "Epoch 00044 | Loss: 1.8179 | Accuracy: 0.2437\n",
      "Epoch 00045 | Loss: 1.8142 | Accuracy: 0.2417\n",
      "Epoch 00046 | Loss: 1.8110 | Accuracy: 0.2354\n",
      "Epoch 00047 | Loss: 1.7989 | Accuracy: 0.2167\n",
      "Epoch 00048 | Loss: 1.7919 | Accuracy: 0.2500\n",
      "Epoch 00049 | Loss: 1.7791 | Accuracy: 0.2375\n",
      "Epoch 00050 | Loss: 1.7808 | Accuracy: 0.2417\n",
      "Epoch 00051 | Loss: 1.7648 | Accuracy: 0.2375\n",
      "Epoch 00052 | Loss: 1.7600 | Accuracy: 0.2396\n",
      "Epoch 00053 | Loss: 1.7490 | Accuracy: 0.2354\n",
      "Epoch 00054 | Loss: 1.7469 | Accuracy: 0.2542\n",
      "Epoch 00055 | Loss: 1.7343 | Accuracy: 0.2458\n",
      "Epoch 00056 | Loss: 1.7307 | Accuracy: 0.2458\n",
      "Epoch 00057 | Loss: 1.7271 | Accuracy: 0.2396\n",
      "Epoch 00058 | Loss: 1.7157 | Accuracy: 0.2542\n",
      "Epoch 00059 | Loss: 1.7139 | Accuracy: 0.2521\n",
      "Epoch 00060 | Loss: 1.7068 | Accuracy: 0.2500\n",
      "Epoch 00061 | Loss: 1.7022 | Accuracy: 0.2396\n",
      "Epoch 00062 | Loss: 1.6948 | Accuracy: 0.2812\n",
      "Epoch 00063 | Loss: 1.6919 | Accuracy: 0.2729\n",
      "Epoch 00064 | Loss: 1.6839 | Accuracy: 0.2687\n",
      "Epoch 00065 | Loss: 1.6821 | Accuracy: 0.2667\n",
      "Epoch 00066 | Loss: 1.6755 | Accuracy: 0.2562\n",
      "Epoch 00067 | Loss: 1.6723 | Accuracy: 0.2750\n",
      "Epoch 00068 | Loss: 1.6676 | Accuracy: 0.3125\n",
      "Epoch 00069 | Loss: 1.6654 | Accuracy: 0.2896\n",
      "Epoch 00070 | Loss: 1.6603 | Accuracy: 0.2938\n",
      "Epoch 00071 | Loss: 1.6584 | Accuracy: 0.2875\n",
      "Epoch 00072 | Loss: 1.6536 | Accuracy: 0.2875\n",
      "Epoch 00073 | Loss: 1.6506 | Accuracy: 0.3021\n",
      "Epoch 00074 | Loss: 1.6467 | Accuracy: 0.3083\n",
      "Epoch 00075 | Loss: 1.6435 | Accuracy: 0.2792\n",
      "Epoch 00076 | Loss: 1.6392 | Accuracy: 0.2938\n",
      "Epoch 00077 | Loss: 1.6364 | Accuracy: 0.3000\n",
      "Epoch 00078 | Loss: 1.6323 | Accuracy: 0.2979\n",
      "Epoch 00079 | Loss: 1.6286 | Accuracy: 0.2875\n",
      "Epoch 00080 | Loss: 1.6259 | Accuracy: 0.3104\n",
      "Epoch 00081 | Loss: 1.6227 | Accuracy: 0.3187\n",
      "Epoch 00082 | Loss: 1.6196 | Accuracy: 0.3083\n",
      "Epoch 00083 | Loss: 1.6167 | Accuracy: 0.3125\n",
      "Epoch 00084 | Loss: 1.6142 | Accuracy: 0.3104\n",
      "Epoch 00085 | Loss: 1.6109 | Accuracy: 0.3333\n",
      "Epoch 00086 | Loss: 1.6084 | Accuracy: 0.3208\n",
      "Epoch 00087 | Loss: 1.6053 | Accuracy: 0.3229\n",
      "Epoch 00088 | Loss: 1.6027 | Accuracy: 0.3250\n",
      "Epoch 00089 | Loss: 1.5999 | Accuracy: 0.3333\n",
      "Epoch 00090 | Loss: 1.5970 | Accuracy: 0.3312\n",
      "Epoch 00091 | Loss: 1.5946 | Accuracy: 0.3396\n",
      "Epoch 00092 | Loss: 1.5917 | Accuracy: 0.3333\n",
      "Epoch 00093 | Loss: 1.5889 | Accuracy: 0.3500\n",
      "Epoch 00094 | Loss: 1.5859 | Accuracy: 0.3479\n",
      "Epoch 00095 | Loss: 1.5833 | Accuracy: 0.3458\n",
      "Epoch 00096 | Loss: 1.5807 | Accuracy: 0.3500\n",
      "Epoch 00097 | Loss: 1.5779 | Accuracy: 0.3458\n",
      "Epoch 00098 | Loss: 1.5752 | Accuracy: 0.3563\n",
      "Epoch 00099 | Loss: 1.5728 | Accuracy: 0.3625\n",
      "Epoch 00100 | Loss: 1.5702 | Accuracy: 0.3521\n",
      "Epoch 00101 | Loss: 1.5674 | Accuracy: 0.3604\n",
      "Epoch 00102 | Loss: 1.5648 | Accuracy: 0.3583\n",
      "Epoch 00103 | Loss: 1.5624 | Accuracy: 0.3646\n",
      "Epoch 00104 | Loss: 1.5599 | Accuracy: 0.3729\n",
      "Epoch 00105 | Loss: 1.5573 | Accuracy: 0.3708\n",
      "Epoch 00106 | Loss: 1.5549 | Accuracy: 0.3667\n",
      "Epoch 00107 | Loss: 1.5523 | Accuracy: 0.3792\n",
      "Epoch 00108 | Loss: 1.5497 | Accuracy: 0.3792\n",
      "Epoch 00109 | Loss: 1.5472 | Accuracy: 0.3750\n",
      "Epoch 00110 | Loss: 1.5447 | Accuracy: 0.3896\n",
      "Epoch 00111 | Loss: 1.5423 | Accuracy: 0.3812\n",
      "Epoch 00112 | Loss: 1.5400 | Accuracy: 0.4000\n",
      "Epoch 00113 | Loss: 1.5378 | Accuracy: 0.3854\n",
      "Epoch 00114 | Loss: 1.5358 | Accuracy: 0.4104\n",
      "Epoch 00115 | Loss: 1.5348 | Accuracy: 0.3937\n",
      "Epoch 00116 | Loss: 1.5352 | Accuracy: 0.4208\n",
      "Epoch 00117 | Loss: 1.5389 | Accuracy: 0.3688\n",
      "Epoch 00118 | Loss: 1.5451 | Accuracy: 0.3979\n",
      "Epoch 00119 | Loss: 1.5546 | Accuracy: 0.3729\n",
      "Epoch 00120 | Loss: 1.5549 | Accuracy: 0.3896\n",
      "Epoch 00121 | Loss: 1.5390 | Accuracy: 0.3792\n",
      "Epoch 00122 | Loss: 1.5220 | Accuracy: 0.4208\n",
      "Epoch 00123 | Loss: 1.5175 | Accuracy: 0.4042\n",
      "Epoch 00124 | Loss: 1.5226 | Accuracy: 0.3896\n",
      "Epoch 00125 | Loss: 1.5329 | Accuracy: 0.4104\n",
      "Epoch 00126 | Loss: 1.5393 | Accuracy: 0.3729\n",
      "Epoch 00127 | Loss: 1.5253 | Accuracy: 0.4104\n",
      "Epoch 00128 | Loss: 1.5101 | Accuracy: 0.4167\n",
      "Epoch 00129 | Loss: 1.5078 | Accuracy: 0.4229\n",
      "Epoch 00130 | Loss: 1.5140 | Accuracy: 0.4208\n",
      "Epoch 00131 | Loss: 1.5238 | Accuracy: 0.3771\n",
      "Epoch 00132 | Loss: 1.5215 | Accuracy: 0.4208\n",
      "Epoch 00133 | Loss: 1.5055 | Accuracy: 0.4188\n",
      "Epoch 00134 | Loss: 1.4976 | Accuracy: 0.4188\n",
      "Epoch 00135 | Loss: 1.5004 | Accuracy: 0.4271\n",
      "Epoch 00136 | Loss: 1.5092 | Accuracy: 0.3937\n",
      "Epoch 00137 | Loss: 1.5154 | Accuracy: 0.4292\n",
      "Epoch 00138 | Loss: 1.5051 | Accuracy: 0.3875\n",
      "Epoch 00139 | Loss: 1.4915 | Accuracy: 0.4250\n",
      "Epoch 00140 | Loss: 1.4885 | Accuracy: 0.4146\n",
      "Epoch 00141 | Loss: 1.4934 | Accuracy: 0.4042\n",
      "Epoch 00142 | Loss: 1.5010 | Accuracy: 0.4229\n",
      "Epoch 00143 | Loss: 1.5001 | Accuracy: 0.3979\n",
      "Epoch 00144 | Loss: 1.4880 | Accuracy: 0.4250\n",
      "Epoch 00145 | Loss: 1.4805 | Accuracy: 0.4250\n",
      "Epoch 00146 | Loss: 1.4780 | Accuracy: 0.4229\n",
      "Epoch 00147 | Loss: 1.4800 | Accuracy: 0.4313\n",
      "Epoch 00148 | Loss: 1.4825 | Accuracy: 0.4042\n",
      "Epoch 00149 | Loss: 1.4815 | Accuracy: 0.4271\n",
      "Epoch 00150 | Loss: 1.4788 | Accuracy: 0.4188\n",
      "Epoch 00151 | Loss: 1.4748 | Accuracy: 0.4375\n",
      "Epoch 00152 | Loss: 1.4709 | Accuracy: 0.4375\n",
      "Epoch 00153 | Loss: 1.4672 | Accuracy: 0.4313\n",
      "Epoch 00154 | Loss: 1.4649 | Accuracy: 0.4313\n",
      "Epoch 00155 | Loss: 1.4641 | Accuracy: 0.4458\n",
      "Epoch 00156 | Loss: 1.4640 | Accuracy: 0.4354\n",
      "Epoch 00157 | Loss: 1.4651 | Accuracy: 0.4271\n",
      "Epoch 00158 | Loss: 1.4656 | Accuracy: 0.4354\n",
      "Epoch 00159 | Loss: 1.4666 | Accuracy: 0.4271\n",
      "Epoch 00160 | Loss: 1.4671 | Accuracy: 0.4458\n",
      "Epoch 00161 | Loss: 1.4684 | Accuracy: 0.4167\n",
      "Epoch 00162 | Loss: 1.4674 | Accuracy: 0.4354\n",
      "Epoch 00163 | Loss: 1.4637 | Accuracy: 0.4313\n",
      "Epoch 00164 | Loss: 1.4593 | Accuracy: 0.4437\n",
      "Epoch 00165 | Loss: 1.4559 | Accuracy: 0.4437\n",
      "Epoch 00166 | Loss: 1.4531 | Accuracy: 0.4500\n",
      "Epoch 00167 | Loss: 1.4508 | Accuracy: 0.4458\n",
      "Epoch 00168 | Loss: 1.4487 | Accuracy: 0.4542\n",
      "Epoch 00169 | Loss: 1.4471 | Accuracy: 0.4521\n",
      "Epoch 00170 | Loss: 1.4455 | Accuracy: 0.4562\n",
      "Epoch 00171 | Loss: 1.4443 | Accuracy: 0.4583\n",
      "Epoch 00172 | Loss: 1.4432 | Accuracy: 0.4479\n",
      "Epoch 00173 | Loss: 1.4423 | Accuracy: 0.4521\n",
      "Epoch 00174 | Loss: 1.4418 | Accuracy: 0.4500\n",
      "Epoch 00175 | Loss: 1.4422 | Accuracy: 0.4521\n",
      "Epoch 00176 | Loss: 1.4438 | Accuracy: 0.4542\n",
      "Epoch 00177 | Loss: 1.4481 | Accuracy: 0.4354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00178 | Loss: 1.4552 | Accuracy: 0.4458\n",
      "Epoch 00179 | Loss: 1.4684 | Accuracy: 0.4146\n",
      "Epoch 00180 | Loss: 1.4756 | Accuracy: 0.4437\n",
      "Epoch 00181 | Loss: 1.4737 | Accuracy: 0.4021\n",
      "Epoch 00182 | Loss: 1.4531 | Accuracy: 0.4500\n",
      "Epoch 00183 | Loss: 1.4363 | Accuracy: 0.4625\n",
      "Epoch 00184 | Loss: 1.4299 | Accuracy: 0.4500\n",
      "Epoch 00185 | Loss: 1.4318 | Accuracy: 0.4583\n",
      "Epoch 00186 | Loss: 1.4404 | Accuracy: 0.4458\n",
      "Epoch 00187 | Loss: 1.4523 | Accuracy: 0.4542\n",
      "Epoch 00188 | Loss: 1.4641 | Accuracy: 0.4104\n",
      "Epoch 00189 | Loss: 1.4581 | Accuracy: 0.4417\n",
      "Epoch 00190 | Loss: 1.4414 | Accuracy: 0.4375\n",
      "Epoch 00191 | Loss: 1.4250 | Accuracy: 0.4521\n",
      "Epoch 00192 | Loss: 1.4215 | Accuracy: 0.4500\n",
      "Epoch 00193 | Loss: 1.4281 | Accuracy: 0.4646\n",
      "Epoch 00194 | Loss: 1.4425 | Accuracy: 0.4562\n",
      "Epoch 00195 | Loss: 1.4567 | Accuracy: 0.4167\n",
      "Epoch 00196 | Loss: 1.4484 | Accuracy: 0.4458\n",
      "Epoch 00197 | Loss: 1.4305 | Accuracy: 0.4313\n",
      "Epoch 00198 | Loss: 1.4150 | Accuracy: 0.4604\n",
      "Epoch 00199 | Loss: 1.4132 | Accuracy: 0.4542\n",
      "Epoch 00200 | Loss: 1.4246 | Accuracy: 0.4542\n",
      "Epoch 00201 | Loss: 1.4402 | Accuracy: 0.4562\n",
      "Epoch 00202 | Loss: 1.4542 | Accuracy: 0.4083\n",
      "Epoch 00203 | Loss: 1.4406 | Accuracy: 0.4479\n",
      "Epoch 00204 | Loss: 1.4202 | Accuracy: 0.4542\n",
      "Epoch 00205 | Loss: 1.4070 | Accuracy: 0.4667\n",
      "Epoch 00206 | Loss: 1.4127 | Accuracy: 0.4479\n",
      "Epoch 00207 | Loss: 1.4296 | Accuracy: 0.4333\n",
      "Epoch 00208 | Loss: 1.4454 | Accuracy: 0.4542\n",
      "Epoch 00209 | Loss: 1.4543 | Accuracy: 0.4083\n",
      "Epoch 00210 | Loss: 1.4210 | Accuracy: 0.4562\n",
      "Epoch 00211 | Loss: 1.4045 | Accuracy: 0.4771\n",
      "Epoch 00212 | Loss: 1.4178 | Accuracy: 0.4417\n",
      "Epoch 00213 | Loss: 1.4315 | Accuracy: 0.4604\n",
      "Epoch 00214 | Loss: 1.4441 | Accuracy: 0.4271\n",
      "Epoch 00215 | Loss: 1.4151 | Accuracy: 0.4729\n",
      "Epoch 00216 | Loss: 1.3928 | Accuracy: 0.4813\n",
      "Epoch 00217 | Loss: 1.4089 | Accuracy: 0.4583\n",
      "Epoch 00218 | Loss: 1.4361 | Accuracy: 0.4521\n",
      "Epoch 00219 | Loss: 1.4463 | Accuracy: 0.4188\n",
      "Epoch 00220 | Loss: 1.3932 | Accuracy: 0.4771\n",
      "Epoch 00221 | Loss: 1.4081 | Accuracy: 0.4688\n",
      "Epoch 00222 | Loss: 1.4808 | Accuracy: 0.3875\n",
      "Epoch 00223 | Loss: 1.5173 | Accuracy: 0.4271\n",
      "Epoch 00224 | Loss: 1.4296 | Accuracy: 0.4354\n",
      "Epoch 00225 | Loss: 1.4961 | Accuracy: 0.3937\n",
      "Epoch 00226 | Loss: 1.6636 | Accuracy: 0.3604\n",
      "Epoch 00227 | Loss: 1.5804 | Accuracy: 0.3812\n",
      "Epoch 00228 | Loss: 1.6669 | Accuracy: 0.3542\n",
      "Epoch 00229 | Loss: 1.5853 | Accuracy: 0.3333\n",
      "Epoch 00230 | Loss: 1.5082 | Accuracy: 0.4354\n",
      "Epoch 00231 | Loss: 1.5104 | Accuracy: 0.4313\n",
      "Epoch 00232 | Loss: 1.4039 | Accuracy: 0.4708\n",
      "Epoch 00233 | Loss: 1.5107 | Accuracy: 0.3625\n",
      "Epoch 00234 | Loss: 1.5584 | Accuracy: 0.4208\n",
      "Epoch 00235 | Loss: 1.5435 | Accuracy: 0.4208\n",
      "Epoch 00236 | Loss: 1.5833 | Accuracy: 0.4042\n",
      "Epoch 00237 | Loss: 1.4512 | Accuracy: 0.4396\n",
      "Epoch 00238 | Loss: 1.5917 | Accuracy: 0.3792\n",
      "Epoch 00239 | Loss: 1.4910 | Accuracy: 0.3854\n",
      "Epoch 00240 | Loss: 1.5166 | Accuracy: 0.4000\n",
      "Epoch 00241 | Loss: 1.5845 | Accuracy: 0.4125\n",
      "Epoch 00242 | Loss: 1.4796 | Accuracy: 0.4313\n",
      "Epoch 00243 | Loss: 1.7349 | Accuracy: 0.2750\n",
      "Epoch 00244 | Loss: 1.4992 | Accuracy: 0.4562\n",
      "Epoch 00245 | Loss: 1.5830 | Accuracy: 0.4188\n",
      "Epoch 00246 | Loss: 1.5172 | Accuracy: 0.4375\n",
      "Epoch 00247 | Loss: 1.4584 | Accuracy: 0.3979\n",
      "Epoch 00248 | Loss: 1.6238 | Accuracy: 0.3688\n",
      "Epoch 00249 | Loss: 1.4258 | Accuracy: 0.4437\n",
      "Epoch 00250 | Loss: 1.4383 | Accuracy: 0.4208\n",
      "Epoch 00251 | Loss: 1.4731 | Accuracy: 0.4396\n",
      "Epoch 00252 | Loss: 1.4341 | Accuracy: 0.4542\n",
      "Epoch 00253 | Loss: 1.4345 | Accuracy: 0.4313\n",
      "Epoch 00254 | Loss: 1.4259 | Accuracy: 0.4500\n",
      "Epoch 00255 | Loss: 1.3988 | Accuracy: 0.4729\n",
      "Epoch 00256 | Loss: 1.4026 | Accuracy: 0.4500\n",
      "Epoch 00257 | Loss: 1.4132 | Accuracy: 0.4458\n",
      "Epoch 00258 | Loss: 1.3945 | Accuracy: 0.4667\n",
      "Epoch 00259 | Loss: 1.4222 | Accuracy: 0.4458\n",
      "Epoch 00260 | Loss: 1.3816 | Accuracy: 0.4813\n",
      "Epoch 00261 | Loss: 1.3836 | Accuracy: 0.4542\n",
      "Epoch 00262 | Loss: 1.3859 | Accuracy: 0.4688\n",
      "Epoch 00263 | Loss: 1.3602 | Accuracy: 0.4979\n",
      "Epoch 00264 | Loss: 1.4057 | Accuracy: 0.4292\n",
      "Epoch 00265 | Loss: 1.3671 | Accuracy: 0.4792\n",
      "Epoch 00266 | Loss: 1.3611 | Accuracy: 0.4792\n",
      "Epoch 00267 | Loss: 1.3782 | Accuracy: 0.4625\n",
      "Epoch 00268 | Loss: 1.3476 | Accuracy: 0.4938\n",
      "Epoch 00269 | Loss: 1.3698 | Accuracy: 0.4854\n",
      "Epoch 00270 | Loss: 1.3617 | Accuracy: 0.4813\n",
      "Epoch 00271 | Loss: 1.3514 | Accuracy: 0.4958\n",
      "Epoch 00272 | Loss: 1.3580 | Accuracy: 0.4771\n",
      "Epoch 00273 | Loss: 1.3416 | Accuracy: 0.5042\n",
      "Epoch 00274 | Loss: 1.3530 | Accuracy: 0.4917\n",
      "Epoch 00275 | Loss: 1.3445 | Accuracy: 0.4917\n",
      "Epoch 00276 | Loss: 1.3411 | Accuracy: 0.5000\n",
      "Epoch 00277 | Loss: 1.3497 | Accuracy: 0.4917\n",
      "Epoch 00278 | Loss: 1.3371 | Accuracy: 0.5104\n",
      "Epoch 00279 | Loss: 1.3393 | Accuracy: 0.4813\n",
      "Epoch 00280 | Loss: 1.3368 | Accuracy: 0.4958\n",
      "Epoch 00281 | Loss: 1.3319 | Accuracy: 0.5021\n",
      "Epoch 00282 | Loss: 1.3353 | Accuracy: 0.4917\n",
      "Epoch 00283 | Loss: 1.3282 | Accuracy: 0.5062\n",
      "Epoch 00284 | Loss: 1.3303 | Accuracy: 0.4979\n",
      "Epoch 00285 | Loss: 1.3300 | Accuracy: 0.5021\n",
      "Epoch 00286 | Loss: 1.3249 | Accuracy: 0.5125\n",
      "Epoch 00287 | Loss: 1.3263 | Accuracy: 0.4958\n",
      "Epoch 00288 | Loss: 1.3236 | Accuracy: 0.5104\n",
      "Epoch 00289 | Loss: 1.3223 | Accuracy: 0.5042\n",
      "Epoch 00290 | Loss: 1.3214 | Accuracy: 0.5042\n",
      "Epoch 00291 | Loss: 1.3193 | Accuracy: 0.5167\n",
      "Epoch 00292 | Loss: 1.3187 | Accuracy: 0.5146\n",
      "Epoch 00293 | Loss: 1.3174 | Accuracy: 0.5083\n",
      "Epoch 00294 | Loss: 1.3159 | Accuracy: 0.5104\n",
      "Epoch 00295 | Loss: 1.3141 | Accuracy: 0.5125\n",
      "Epoch 00296 | Loss: 1.3140 | Accuracy: 0.5021\n",
      "Epoch 00297 | Loss: 1.3121 | Accuracy: 0.5104\n",
      "Epoch 00298 | Loss: 1.3108 | Accuracy: 0.5125\n",
      "Epoch 00299 | Loss: 1.3101 | Accuracy: 0.5062\n",
      "Epoch 00300 | Loss: 1.3078 | Accuracy: 0.5167\n",
      "Epoch 00301 | Loss: 1.3075 | Accuracy: 0.5146\n",
      "Epoch 00302 | Loss: 1.3062 | Accuracy: 0.5146\n",
      "Epoch 00303 | Loss: 1.3044 | Accuracy: 0.5125\n",
      "Epoch 00304 | Loss: 1.3035 | Accuracy: 0.5146\n",
      "Epoch 00305 | Loss: 1.3022 | Accuracy: 0.5104\n",
      "Epoch 00306 | Loss: 1.3009 | Accuracy: 0.5167\n",
      "Epoch 00307 | Loss: 1.2996 | Accuracy: 0.5188\n",
      "Epoch 00308 | Loss: 1.2984 | Accuracy: 0.5146\n",
      "Epoch 00309 | Loss: 1.2969 | Accuracy: 0.5146\n",
      "Epoch 00310 | Loss: 1.2959 | Accuracy: 0.5167\n",
      "Epoch 00311 | Loss: 1.2948 | Accuracy: 0.5125\n",
      "Epoch 00312 | Loss: 1.2932 | Accuracy: 0.5125\n",
      "Epoch 00313 | Loss: 1.2919 | Accuracy: 0.5188\n",
      "Epoch 00314 | Loss: 1.2907 | Accuracy: 0.5188\n",
      "Epoch 00315 | Loss: 1.2895 | Accuracy: 0.5188\n",
      "Epoch 00316 | Loss: 1.2879 | Accuracy: 0.5188\n",
      "Epoch 00317 | Loss: 1.2867 | Accuracy: 0.5167\n",
      "Epoch 00318 | Loss: 1.2855 | Accuracy: 0.5188\n",
      "Epoch 00319 | Loss: 1.2840 | Accuracy: 0.5208\n",
      "Epoch 00320 | Loss: 1.2824 | Accuracy: 0.5229\n",
      "Epoch 00321 | Loss: 1.2810 | Accuracy: 0.5229\n",
      "Epoch 00322 | Loss: 1.2798 | Accuracy: 0.5188\n",
      "Epoch 00323 | Loss: 1.2786 | Accuracy: 0.5271\n",
      "Epoch 00324 | Loss: 1.2772 | Accuracy: 0.5167\n",
      "Epoch 00325 | Loss: 1.2758 | Accuracy: 0.5229\n",
      "Epoch 00326 | Loss: 1.2744 | Accuracy: 0.5167\n",
      "Epoch 00327 | Loss: 1.2729 | Accuracy: 0.5292\n",
      "Epoch 00328 | Loss: 1.2714 | Accuracy: 0.5229\n",
      "Epoch 00329 | Loss: 1.2698 | Accuracy: 0.5271\n",
      "Epoch 00330 | Loss: 1.2682 | Accuracy: 0.5229\n",
      "Epoch 00331 | Loss: 1.2668 | Accuracy: 0.5250\n",
      "Epoch 00332 | Loss: 1.2655 | Accuracy: 0.5167\n",
      "Epoch 00333 | Loss: 1.2642 | Accuracy: 0.5188\n",
      "Epoch 00334 | Loss: 1.2634 | Accuracy: 0.5188\n",
      "Epoch 00335 | Loss: 1.2637 | Accuracy: 0.5167\n",
      "Epoch 00336 | Loss: 1.2653 | Accuracy: 0.5062\n",
      "Epoch 00337 | Loss: 1.2685 | Accuracy: 0.5229\n",
      "Epoch 00338 | Loss: 1.2722 | Accuracy: 0.4958\n",
      "Epoch 00339 | Loss: 1.2748 | Accuracy: 0.5250\n",
      "Epoch 00340 | Loss: 1.2717 | Accuracy: 0.4938\n",
      "Epoch 00341 | Loss: 1.2663 | Accuracy: 0.5375\n",
      "Epoch 00342 | Loss: 1.2599 | Accuracy: 0.5125\n",
      "Epoch 00343 | Loss: 1.2535 | Accuracy: 0.5292\n",
      "Epoch 00344 | Loss: 1.2495 | Accuracy: 0.5188\n",
      "Epoch 00345 | Loss: 1.2479 | Accuracy: 0.5188\n",
      "Epoch 00346 | Loss: 1.2484 | Accuracy: 0.5229\n",
      "Epoch 00347 | Loss: 1.2497 | Accuracy: 0.5188\n",
      "Epoch 00348 | Loss: 1.2518 | Accuracy: 0.5271\n",
      "Epoch 00349 | Loss: 1.2544 | Accuracy: 0.5042\n",
      "Epoch 00350 | Loss: 1.2584 | Accuracy: 0.5250\n",
      "Epoch 00351 | Loss: 1.2620 | Accuracy: 0.4979\n",
      "Epoch 00352 | Loss: 1.2647 | Accuracy: 0.5188\n",
      "Epoch 00353 | Loss: 1.2593 | Accuracy: 0.4979\n",
      "Epoch 00354 | Loss: 1.2510 | Accuracy: 0.5312\n",
      "Epoch 00355 | Loss: 1.2429 | Accuracy: 0.5125\n",
      "Epoch 00356 | Loss: 1.2379 | Accuracy: 0.5271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00357 | Loss: 1.2349 | Accuracy: 0.5208\n",
      "Epoch 00358 | Loss: 1.2323 | Accuracy: 0.5250\n",
      "Epoch 00359 | Loss: 1.2307 | Accuracy: 0.5250\n",
      "Epoch 00360 | Loss: 1.2307 | Accuracy: 0.5188\n",
      "Epoch 00361 | Loss: 1.2321 | Accuracy: 0.5396\n",
      "Epoch 00362 | Loss: 1.2347 | Accuracy: 0.5229\n",
      "Epoch 00363 | Loss: 1.2392 | Accuracy: 0.5417\n",
      "Epoch 00364 | Loss: 1.2479 | Accuracy: 0.5083\n",
      "Epoch 00365 | Loss: 1.2632 | Accuracy: 0.5333\n",
      "Epoch 00366 | Loss: 1.2692 | Accuracy: 0.4917\n",
      "Epoch 00367 | Loss: 1.2585 | Accuracy: 0.5250\n",
      "Epoch 00368 | Loss: 1.2328 | Accuracy: 0.5062\n",
      "Epoch 00369 | Loss: 1.2207 | Accuracy: 0.5292\n",
      "Epoch 00370 | Loss: 1.2248 | Accuracy: 0.5208\n",
      "Epoch 00371 | Loss: 1.2375 | Accuracy: 0.5167\n",
      "Epoch 00372 | Loss: 1.2563 | Accuracy: 0.5354\n",
      "Epoch 00373 | Loss: 1.2693 | Accuracy: 0.5021\n",
      "Epoch 00374 | Loss: 1.2571 | Accuracy: 0.5167\n",
      "Epoch 00375 | Loss: 1.2338 | Accuracy: 0.5208\n",
      "Epoch 00376 | Loss: 1.2170 | Accuracy: 0.5292\n",
      "Epoch 00377 | Loss: 1.2264 | Accuracy: 0.5250\n",
      "Epoch 00378 | Loss: 1.2603 | Accuracy: 0.5062\n",
      "Epoch 00379 | Loss: 1.3154 | Accuracy: 0.4771\n",
      "Epoch 00380 | Loss: 1.3230 | Accuracy: 0.4583\n",
      "Epoch 00381 | Loss: 1.2219 | Accuracy: 0.5312\n",
      "Epoch 00382 | Loss: 1.2743 | Accuracy: 0.4938\n",
      "Epoch 00383 | Loss: 1.4261 | Accuracy: 0.4146\n",
      "Epoch 00384 | Loss: 1.4057 | Accuracy: 0.4542\n",
      "Epoch 00385 | Loss: 1.4187 | Accuracy: 0.4208\n",
      "Epoch 00386 | Loss: 1.6496 | Accuracy: 0.4000\n",
      "Epoch 00387 | Loss: 1.3626 | Accuracy: 0.4771\n",
      "Epoch 00388 | Loss: 1.5518 | Accuracy: 0.4000\n",
      "Epoch 00389 | Loss: 1.4287 | Accuracy: 0.4875\n",
      "Epoch 00390 | Loss: 1.5584 | Accuracy: 0.4583\n",
      "Epoch 00391 | Loss: 1.5791 | Accuracy: 0.4062\n",
      "Epoch 00392 | Loss: 1.6077 | Accuracy: 0.3646\n",
      "Epoch 00393 | Loss: 1.8722 | Accuracy: 0.3896\n",
      "Epoch 00394 | Loss: 1.3666 | Accuracy: 0.4646\n",
      "Epoch 00395 | Loss: 2.0380 | Accuracy: 0.3396\n",
      "Epoch 00396 | Loss: 1.5069 | Accuracy: 0.4104\n",
      "Epoch 00397 | Loss: 1.9000 | Accuracy: 0.3604\n",
      "Epoch 00398 | Loss: 1.4179 | Accuracy: 0.4229\n",
      "Epoch 00399 | Loss: 1.5629 | Accuracy: 0.4104\n",
      "Epoch 00400 | Loss: 1.7565 | Accuracy: 0.3646\n",
      "Epoch 00401 | Loss: 1.4515 | Accuracy: 0.4729\n",
      "Epoch 00402 | Loss: 1.3704 | Accuracy: 0.4646\n",
      "Epoch 00403 | Loss: 1.4918 | Accuracy: 0.3875\n",
      "Epoch 00404 | Loss: 1.4083 | Accuracy: 0.4521\n",
      "Epoch 00405 | Loss: 1.5878 | Accuracy: 0.4354\n",
      "Epoch 00406 | Loss: 1.3924 | Accuracy: 0.4292\n",
      "Epoch 00407 | Loss: 1.3705 | Accuracy: 0.4417\n",
      "Epoch 00408 | Loss: 1.4859 | Accuracy: 0.4375\n",
      "Epoch 00409 | Loss: 1.2727 | Accuracy: 0.4938\n",
      "Epoch 00410 | Loss: 1.4376 | Accuracy: 0.4479\n",
      "Epoch 00411 | Loss: 1.3062 | Accuracy: 0.4854\n",
      "Epoch 00412 | Loss: 1.2536 | Accuracy: 0.5104\n",
      "Epoch 00413 | Loss: 1.3541 | Accuracy: 0.4542\n",
      "Epoch 00414 | Loss: 1.2283 | Accuracy: 0.5292\n",
      "Epoch 00415 | Loss: 1.2886 | Accuracy: 0.4604\n",
      "Epoch 00416 | Loss: 1.3295 | Accuracy: 0.4813\n",
      "Epoch 00417 | Loss: 1.2299 | Accuracy: 0.5312\n",
      "Epoch 00418 | Loss: 1.3102 | Accuracy: 0.4854\n",
      "Epoch 00419 | Loss: 1.2288 | Accuracy: 0.5062\n",
      "Epoch 00420 | Loss: 1.2519 | Accuracy: 0.5042\n",
      "Epoch 00421 | Loss: 1.2893 | Accuracy: 0.5146\n",
      "Epoch 00422 | Loss: 1.2124 | Accuracy: 0.5333\n",
      "Epoch 00423 | Loss: 1.2630 | Accuracy: 0.4958\n",
      "Epoch 00424 | Loss: 1.2241 | Accuracy: 0.5312\n",
      "Epoch 00425 | Loss: 1.2090 | Accuracy: 0.5437\n",
      "Epoch 00426 | Loss: 1.2519 | Accuracy: 0.5042\n",
      "Epoch 00427 | Loss: 1.1955 | Accuracy: 0.5354\n",
      "Epoch 00428 | Loss: 1.2189 | Accuracy: 0.5354\n",
      "Epoch 00429 | Loss: 1.2168 | Accuracy: 0.5062\n",
      "Epoch 00430 | Loss: 1.1924 | Accuracy: 0.5271\n",
      "Epoch 00431 | Loss: 1.2264 | Accuracy: 0.5146\n",
      "Epoch 00432 | Loss: 1.2093 | Accuracy: 0.5208\n",
      "Epoch 00433 | Loss: 1.1914 | Accuracy: 0.5250\n",
      "Epoch 00434 | Loss: 1.2025 | Accuracy: 0.5458\n",
      "Epoch 00435 | Loss: 1.1914 | Accuracy: 0.5208\n",
      "Epoch 00436 | Loss: 1.1928 | Accuracy: 0.5250\n",
      "Epoch 00437 | Loss: 1.1960 | Accuracy: 0.5396\n",
      "Epoch 00438 | Loss: 1.1874 | Accuracy: 0.5146\n",
      "Epoch 00439 | Loss: 1.1837 | Accuracy: 0.5146\n",
      "Epoch 00440 | Loss: 1.1843 | Accuracy: 0.5500\n",
      "Epoch 00441 | Loss: 1.1810 | Accuracy: 0.5250\n",
      "Epoch 00442 | Loss: 1.1771 | Accuracy: 0.5292\n",
      "Epoch 00443 | Loss: 1.1820 | Accuracy: 0.5354\n",
      "Epoch 00444 | Loss: 1.1760 | Accuracy: 0.5292\n",
      "Epoch 00445 | Loss: 1.1726 | Accuracy: 0.5417\n",
      "Epoch 00446 | Loss: 1.1766 | Accuracy: 0.5479\n",
      "Epoch 00447 | Loss: 1.1703 | Accuracy: 0.5437\n",
      "Epoch 00448 | Loss: 1.1704 | Accuracy: 0.5354\n",
      "Epoch 00449 | Loss: 1.1714 | Accuracy: 0.5437\n",
      "Epoch 00450 | Loss: 1.1682 | Accuracy: 0.5479\n",
      "Epoch 00451 | Loss: 1.1667 | Accuracy: 0.5417\n",
      "Epoch 00452 | Loss: 1.1667 | Accuracy: 0.5479\n",
      "Epoch 00453 | Loss: 1.1660 | Accuracy: 0.5354\n",
      "Epoch 00454 | Loss: 1.1626 | Accuracy: 0.5417\n",
      "Epoch 00455 | Loss: 1.1633 | Accuracy: 0.5542\n",
      "Epoch 00456 | Loss: 1.1627 | Accuracy: 0.5437\n",
      "Epoch 00457 | Loss: 1.1595 | Accuracy: 0.5458\n",
      "Epoch 00458 | Loss: 1.1598 | Accuracy: 0.5396\n",
      "Epoch 00459 | Loss: 1.1592 | Accuracy: 0.5417\n",
      "Epoch 00460 | Loss: 1.1575 | Accuracy: 0.5437\n",
      "Epoch 00461 | Loss: 1.1561 | Accuracy: 0.5500\n",
      "Epoch 00462 | Loss: 1.1558 | Accuracy: 0.5396\n",
      "Epoch 00463 | Loss: 1.1548 | Accuracy: 0.5521\n",
      "Epoch 00464 | Loss: 1.1532 | Accuracy: 0.5500\n",
      "Epoch 00465 | Loss: 1.1528 | Accuracy: 0.5437\n",
      "Epoch 00466 | Loss: 1.1519 | Accuracy: 0.5542\n",
      "Epoch 00467 | Loss: 1.1508 | Accuracy: 0.5375\n",
      "Epoch 00468 | Loss: 1.1496 | Accuracy: 0.5458\n",
      "Epoch 00469 | Loss: 1.1490 | Accuracy: 0.5542\n",
      "Epoch 00470 | Loss: 1.1483 | Accuracy: 0.5437\n",
      "Epoch 00471 | Loss: 1.1470 | Accuracy: 0.5521\n",
      "Epoch 00472 | Loss: 1.1463 | Accuracy: 0.5479\n",
      "Epoch 00473 | Loss: 1.1455 | Accuracy: 0.5437\n",
      "Epoch 00474 | Loss: 1.1446 | Accuracy: 0.5500\n",
      "Epoch 00475 | Loss: 1.1435 | Accuracy: 0.5479\n",
      "Epoch 00476 | Loss: 1.1427 | Accuracy: 0.5458\n",
      "Epoch 00477 | Loss: 1.1420 | Accuracy: 0.5521\n",
      "Epoch 00478 | Loss: 1.1411 | Accuracy: 0.5521\n",
      "Epoch 00479 | Loss: 1.1401 | Accuracy: 0.5521\n",
      "Epoch 00480 | Loss: 1.1392 | Accuracy: 0.5521\n",
      "Epoch 00481 | Loss: 1.1385 | Accuracy: 0.5458\n",
      "Epoch 00482 | Loss: 1.1376 | Accuracy: 0.5521\n",
      "Epoch 00483 | Loss: 1.1366 | Accuracy: 0.5542\n",
      "Epoch 00484 | Loss: 1.1358 | Accuracy: 0.5521\n",
      "Epoch 00485 | Loss: 1.1350 | Accuracy: 0.5521\n",
      "Epoch 00486 | Loss: 1.1341 | Accuracy: 0.5500\n",
      "Epoch 00487 | Loss: 1.1331 | Accuracy: 0.5521\n",
      "Epoch 00488 | Loss: 1.1322 | Accuracy: 0.5500\n",
      "Epoch 00489 | Loss: 1.1312 | Accuracy: 0.5521\n",
      "Epoch 00490 | Loss: 1.1303 | Accuracy: 0.5521\n",
      "Epoch 00491 | Loss: 1.1294 | Accuracy: 0.5479\n",
      "Epoch 00492 | Loss: 1.1284 | Accuracy: 0.5521\n",
      "Epoch 00493 | Loss: 1.1274 | Accuracy: 0.5479\n",
      "Epoch 00494 | Loss: 1.1264 | Accuracy: 0.5500\n",
      "Epoch 00495 | Loss: 1.1254 | Accuracy: 0.5500\n",
      "Epoch 00496 | Loss: 1.1245 | Accuracy: 0.5479\n",
      "Epoch 00497 | Loss: 1.1235 | Accuracy: 0.5542\n",
      "Epoch 00498 | Loss: 1.1224 | Accuracy: 0.5542\n",
      "Epoch 00499 | Loss: 1.1213 | Accuracy: 0.5563\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    loss_list = []\n",
    "    true_samples = 0\n",
    "    num_samples = 0\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "        bg, labels = batch_data\n",
    "        atom_feats = bg.ndata.pop('feat')\n",
    "        atom_feats, labels = atom_feats.to(device), \\\n",
    "                                   labels.to(device)\n",
    "        logits = model(bg, atom_feats)\n",
    "        loss = loss_criterion(logits, labels)\n",
    "        true_samples += (logits.argmax(1)==labels.long()).float().sum().item()\n",
    "        num_samples += len(labels)\n",
    "        loss_list.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch {:05d} | Loss: {:.4f} | Accuracy: {:.4f}\".format(i, np.mean(loss_list), true_samples/num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "true_samples = 0\n",
    "num_samples = 0\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch_data in enumerate(val_loader):\n",
    "        bg, labels = batch_data\n",
    "        atom_feats = bg.ndata.pop('feat')\n",
    "        atom_feats, labels = atom_feats.to(device), \\\n",
    "                                   labels.to(device)\n",
    "        logits = model(bg, atom_feats)\n",
    "        logits.argmax()\n",
    "        num_samples += len(labels)\n",
    "        true_samples += (logits.argmax(1)==labels.long()).float().sum().item()\n",
    "print(true_samples/num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We acheived a pretty high accuracy. However, this dataset's labels are imbalanced, which means most labels could be negative. Therefore it would be unfair to evaluate result with accuracy score. A more detailed analysis of this task could be found at our [model zoo](https://github.com/dmlc/dgl/tree/master/examples/pytorch/model_zoo/chem/property_prediction)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
