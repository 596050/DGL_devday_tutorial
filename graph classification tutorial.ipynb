{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification with DGL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate how to use DGL to finish graph classification tasks. The dataset we use here is Tox21, a public database measuring toxicity of compounds.  The dataset contains qualitative toxicity measurements for 8014 compounds on 12 different targets, including nuclear receptors and stress response pathways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import TUDataset\n",
    "from dgl import model_zoo\n",
    "from dgl.data.utils import split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw,MolFromSmiles, MolToSmiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would take about one minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = TUDataset(\"ENZYMES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.graph_labels=torch.tensor(dataset.graph_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i][0].ndata['feat']= torch.tensor(dataset[i][0].ndata['feat']).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset = split_dataset(dataset, [0.8, 0.2], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGLGraph(num_nodes=37, num_edges=168,\n",
      "         ndata_schemes={'feat': Scheme(shape=(18,), dtype=torch.float32)}\n",
      "         edata_schemes={})\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "graph, label= dataset[0]\n",
    "print(graph)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL could batch multiple small graphs together to accelerate the computation. Detail of batching can be found [here](https://docs.dgl.ai/tutorials/basics/4_batch.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/batch/batch.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_molgraphs_for_classification(data):\n",
    "    \"\"\"Batching a list of datapoints for dataloader in classification tasks.\"\"\"\n",
    "    graphs, labels = map(list, zip(*data))\n",
    "    bg = dgl.batch(graphs)\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "    return bg, labels\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=256,\n",
    "                          collate_fn=collate_molgraphs_for_classification)\n",
    "val_loader = DataLoader(valset, batch_size=256,\n",
    "                        collate_fn=collate_molgraphs_for_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a two layer Graph Convolutional Network to classify the graphs. Detailed source code can be found [here](https://github.com/dmlc/dgl/blob/master/python/dgl/model_zoo/chem/classifiers.py#L111)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNClassifier(\n",
      "  (gnn_layers): ModuleList(\n",
      "    (0): GCNLayer(\n",
      "      (graph_conv): GraphConv(in=18, out=64, normalization=False, activation=<function relu at 0x7f7093160ae8>)\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (res_connection): Linear(in_features=18, out_features=64, bias=True)\n",
      "      (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): GCNLayer(\n",
      "      (graph_conv): GraphConv(in=64, out=64, normalization=False, activation=<function relu at 0x7f7093160ae8>)\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (res_connection): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (bn_layer): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (weighted_sum_readout): WeightAndSum(\n",
      "    (atom_weighting): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=1, bias=True)\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (soft_classifier): MLPBinaryClassifier(\n",
      "    (predict): Sequential(\n",
      "      (0): Dropout(p=0.0)\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (4): Linear(in_features=128, out_features=6, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = model_zoo.chem.GCNClassifier(in_feats=18, gcn_hidden_feats=[64, 64], n_tasks=6).cuda()\n",
    "\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss: 1.8064 | Accuracy: 0.1750\n",
      "Epoch 00001 | Loss: 1.7262 | Accuracy: 0.2562\n",
      "Epoch 00002 | Loss: 1.6770 | Accuracy: 0.3271\n",
      "Epoch 00003 | Loss: 1.6389 | Accuracy: 0.3688\n",
      "Epoch 00004 | Loss: 1.6069 | Accuracy: 0.3937\n",
      "Epoch 00005 | Loss: 1.5790 | Accuracy: 0.4042\n",
      "Epoch 00006 | Loss: 1.5534 | Accuracy: 0.4167\n",
      "Epoch 00007 | Loss: 1.5281 | Accuracy: 0.4104\n",
      "Epoch 00008 | Loss: 1.5024 | Accuracy: 0.4292\n",
      "Epoch 00009 | Loss: 1.4768 | Accuracy: 0.4500\n",
      "Epoch 00010 | Loss: 1.4512 | Accuracy: 0.4646\n",
      "Epoch 00011 | Loss: 1.4260 | Accuracy: 0.4688\n",
      "Epoch 00012 | Loss: 1.3999 | Accuracy: 0.4875\n",
      "Epoch 00013 | Loss: 1.3728 | Accuracy: 0.5042\n",
      "Epoch 00014 | Loss: 1.3454 | Accuracy: 0.5188\n",
      "Epoch 00015 | Loss: 1.3174 | Accuracy: 0.5354\n",
      "Epoch 00016 | Loss: 1.2892 | Accuracy: 0.5500\n",
      "Epoch 00017 | Loss: 1.2607 | Accuracy: 0.5708\n",
      "Epoch 00018 | Loss: 1.2320 | Accuracy: 0.5813\n",
      "Epoch 00019 | Loss: 1.2031 | Accuracy: 0.5979\n",
      "Epoch 00020 | Loss: 1.1734 | Accuracy: 0.6250\n",
      "Epoch 00021 | Loss: 1.1432 | Accuracy: 0.6479\n",
      "Epoch 00022 | Loss: 1.1128 | Accuracy: 0.6708\n",
      "Epoch 00023 | Loss: 1.0837 | Accuracy: 0.6854\n",
      "Epoch 00024 | Loss: 1.0546 | Accuracy: 0.6979\n",
      "Epoch 00025 | Loss: 1.0242 | Accuracy: 0.7125\n",
      "Epoch 00026 | Loss: 0.9932 | Accuracy: 0.7125\n",
      "Epoch 00027 | Loss: 0.9625 | Accuracy: 0.7167\n",
      "Epoch 00028 | Loss: 0.9320 | Accuracy: 0.7333\n",
      "Epoch 00029 | Loss: 0.9017 | Accuracy: 0.7562\n",
      "Epoch 00030 | Loss: 0.8719 | Accuracy: 0.7771\n",
      "Epoch 00031 | Loss: 0.8415 | Accuracy: 0.7896\n",
      "Epoch 00032 | Loss: 0.8129 | Accuracy: 0.8000\n",
      "Epoch 00033 | Loss: 0.7832 | Accuracy: 0.8083\n",
      "Epoch 00034 | Loss: 0.7535 | Accuracy: 0.8167\n",
      "Epoch 00035 | Loss: 0.7237 | Accuracy: 0.8271\n",
      "Epoch 00036 | Loss: 0.6944 | Accuracy: 0.8354\n",
      "Epoch 00037 | Loss: 0.6656 | Accuracy: 0.8458\n",
      "Epoch 00038 | Loss: 0.6374 | Accuracy: 0.8604\n",
      "Epoch 00039 | Loss: 0.6083 | Accuracy: 0.8792\n",
      "Epoch 00040 | Loss: 0.5809 | Accuracy: 0.8750\n",
      "Epoch 00041 | Loss: 0.5548 | Accuracy: 0.8958\n",
      "Epoch 00042 | Loss: 0.5300 | Accuracy: 0.8938\n",
      "Epoch 00043 | Loss: 0.5034 | Accuracy: 0.9083\n",
      "Epoch 00044 | Loss: 0.4781 | Accuracy: 0.9104\n",
      "Epoch 00045 | Loss: 0.4535 | Accuracy: 0.9229\n",
      "Epoch 00046 | Loss: 0.4314 | Accuracy: 0.9271\n",
      "Epoch 00047 | Loss: 0.4077 | Accuracy: 0.9375\n",
      "Epoch 00048 | Loss: 0.3909 | Accuracy: 0.9500\n",
      "Epoch 00049 | Loss: 0.3664 | Accuracy: 0.9542\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    loss_list = []\n",
    "    true_samples = 0\n",
    "    num_samples = 0\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "        bg, labels = batch_data\n",
    "        atom_feats = bg.ndata.pop('feat')\n",
    "        atom_feats, labels = atom_feats.to('cuda'), \\\n",
    "                                   labels.to('cuda')\n",
    "        logits = model(bg, atom_feats)\n",
    "        loss = loss_criterion(logits, labels)\n",
    "        true_samples += (logits.argmax(1)==labels.long()).float().sum().item()\n",
    "        num_samples += len(labels)\n",
    "        loss_list.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch {:05d} | Loss: {:.4f} | Accuracy: {:.4f}\".format(i, np.mean(loss_list), true_samples/num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "true_samples = 0\n",
    "num_samples = 0\n",
    "with torch.no_grad():\n",
    "    for batch_id, batch_data in enumerate(val_loader):\n",
    "        bg, labels = batch_data\n",
    "        atom_feats = bg.ndata.pop('feat')\n",
    "        atom_feats, labels = atom_feats.to('cuda'), \\\n",
    "                                   labels.to('cuda')\n",
    "        logits = model(bg, atom_feats)\n",
    "        logits.argmax()\n",
    "        num_samples += len(labels)\n",
    "        true_samples += (logits.argmax(1)==labels.long()).float().sum().item()\n",
    "print(true_samples/num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We acheived a pretty high accuracy. However, this dataset's labels are imbalanced, which means most labels could be negative. Therefore it would be unfair to evaluate result with accuracy score. A more detailed analysis of this task could be found at our [model zoo](https://github.com/dmlc/dgl/tree/master/examples/pytorch/model_zoo/chem/property_prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_miniconda3-latest)",
   "language": "python",
   "name": "conda_miniconda3-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
